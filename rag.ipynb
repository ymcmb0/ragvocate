{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5a570513",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': 'speech.txt'}, page_content='The world must be made safe for democracy. Its peace must be planted upon the tested foundations of political liberty. We have no selfish ends to serve. We desire no conquest, no dominion. We seek no indemnities for ourselves, no material compensation for the sacrifices we shall freely make. We are but one of the champions of the rights of mankind. We shall be satisfied when those rights have been made as secure as the faith and the freedom of nations can make them.\\n\\nJust because we fight without rancor and without selfish object, seeking nothing for ourselves but what we shall wish to share with all free peoples, we shall, I feel confident, conduct our operations as belligerents without passion and ourselves observe with proud punctilio the principles of right and of fair play we profess to be fighting for.\\n\\nâ€¦\\n\\nIt will be all the easier for us to conduct ourselves as belligerents in a high spirit of right and fairness because we act without animus, not in enmity toward a people or with the desire to bring any injury or disadvantage upon them, but only in armed opposition to an irresponsible government which has thrown aside all considerations of humanity and of right and is running amuck. We are, let me say again, the sincere friends of the German people, and shall desire nothing so much as the early reestablishment of intimate relations of mutual advantage between usâ€”however hard it may be for them, for the time being, to believe that this is spoken from our hearts.\\n\\nWe have borne with their present government through all these bitter months because of that friendshipâ€”exercising a patience and forbearance which would otherwise have been impossible. We shall, happily, still have an opportunity to prove that friendship in our daily attitude and actions toward the millions of men and women of German birth and native sympathy who live among us and share our life, and we shall be proud to prove it toward all who are in fact loyal to their neighbors and to the government in the hour of test. They are, most of them, as true and loyal Americans as if they had never known any other fealty or allegiance. They will be prompt to stand with us in rebuking and restraining the few who may be of a different mind and purpose. If there should be disloyalty, it will be dealt with with a firm hand of stern repression; but, if it lifts its head at all, it will lift it only here and there and without countenance except from a lawless and malignant few.\\n\\nIt is a distressing and oppressive duty, gentlemen of the Congress, which I have performed in thus addressing you. There are, it may be, many months of fiery trial and sacrifice ahead of us. It is a fearful thing to lead this great peaceful people into war, into the most terrible and disastrous of all wars, civilization itself seeming to be in the balance. But the right is more precious than peace, and we shall fight for the things which we have always carried nearest our heartsâ€”for democracy, for the right of those who submit to authority to have a voice in their own governments, for the rights and liberties of small nations, for a universal dominion of right by such a concert of free peoples as shall bring peace and safety to all nations and make the world itself at last free.\\n\\nTo such a task we can dedicate our lives and our fortunes, everything that we are and everything that we have, with the pride of those who know that the day has come when America is privileged to spend her blood and her might for the principles that gave her birth and happiness and the peace which she has treasured. God helping her, she can do no other.')]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# data ingestion\n",
    "from langchain_community.document_loaders import TextLoader\n",
    "loader=TextLoader(\"speech.txt\")\n",
    "text_documents=loader.load()\n",
    "text_documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "981e2382",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LangSmith tracing enabled if environment variables are set correctly.\n",
      "Tracing enabled: true\n",
      "LangSmith Project: ravogate\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "\n",
    "print(\"LangSmith tracing enabled if environment variables are set correctly.\")\n",
    "print(f\"Tracing enabled: {os.environ.get('LANGCHAIN_TRACING_V2')}\")\n",
    "print(f\"LangSmith Project: {os.environ.get('LANGCHAIN_PROJECT')}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bcdce259",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pdf reader\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "loader=PyPDFLoader(\"Legal_Query_RAG_IEEE.pdf\")\n",
    "docs=loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "38c1185d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'producer': 'pdfTeX-1.40.24; modified using iText® Core 7.2.4 (AGPL version) ©2000-2022 iText Group NV', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-02-27T19:55:58+05:30', 'moddate': '2025-02-28T17:25:14-05:00', 'ieee article id': '10887211', 'trapped': 'False', 'ieee issue id': '10820123', 'subject': 'IEEE Access;2025;13; ;10.1109/ACCESS.2025.3542125', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.24 (TeX Live 2022) kpathsea version 6.3.4', 'ieee publication id': '6287639', 'title': 'Legal Query RAG', 'source': 'Legal_Query_RAG_IEEE.pdf', 'total_pages': 17, 'page': 0, 'page_label': '36978'}, page_content='Received 14 January 2025, accepted 31 January 2025, date of publication 14 February 2025, date of current version 3 March 2025.\\nDigital Object Identifier 10.1 109/ACCESS.2025.3542125\\nLegal Query RAG\\nRAHMAN S. M. WAHIDUR\\n1, SUMIN KIM\\n 2, HAEUNG CHOI\\n 1, DAVID S. BHATTI\\n1,\\nAND HEUNG-NO LEE\\n 1, (Senior Member, IEEE)\\n1School of Electrical Engineering and Computer Science, Gwangju Institute of Science and Technology, Gwangju 61005, South Korea'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.24; modified using iText® Core 7.2.4 (AGPL version) ©2000-2022 iText Group NV', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-02-27T19:55:58+05:30', 'moddate': '2025-02-28T17:25:14-05:00', 'ieee article id': '10887211', 'trapped': 'False', 'ieee issue id': '10820123', 'subject': 'IEEE Access;2025;13; ;10.1109/ACCESS.2025.3542125', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.24 (TeX Live 2022) kpathsea version 6.3.4', 'ieee publication id': '6287639', 'title': 'Legal Query RAG', 'source': 'Legal_Query_RAG_IEEE.pdf', 'total_pages': 17, 'page': 0, 'page_label': '36978'}, page_content='1, DAVID S. BHATTI\\n1,\\nAND HEUNG-NO LEE\\n 1, (Senior Member, IEEE)\\n1School of Electrical Engineering and Computer Science, Gwangju Institute of Science and Technology, Gwangju 61005, South Korea\\n2Artificial Intelligence Graduate School, Gwangju Institute of Science and Technology, Gwangju 61005, South Korea\\nCorresponding author: Heung-No Lee (heungno@gist.ac.kr)\\nThis work was supported by Institute of Information & communications Technology Planning & Evaluation (IITP) grant funded by the')]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "text_splitter=RecursiveCharacterTextSplitter(chunk_size=500,chunk_overlap=200)\n",
    "chunks=text_splitter.split_documents(docs)\n",
    "documents=text_splitter.split_documents(docs)\n",
    "documents[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c986dc95",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\computer world\\AppData\\Local\\Temp\\ipykernel_12028\\3872278415.py:5: LangChainDeprecationWarning: The class `HuggingFaceEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-huggingface package and should be used instead. To use it run `pip install -U :class:`~langchain-huggingface` and import as `from :class:`~langchain_huggingface import HuggingFaceEmbeddings``.\n",
      "  embeddings = SentenceTransformerEmbeddings(model_name=\"all-MiniLM-L6-v2\")\n",
      "c:\\Users\\computer world\\Desktop\\Code\\Python\\langchain\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading existing vector store from: ./my_pdf_embeddings\n",
      "Chroma DB created/loaded with 227 documents and persisted to ./my_pdf_embeddings\n"
     ]
    }
   ],
   "source": [
    "# vectorization embedding and vector store\n",
    "from langchain_community.embeddings import SentenceTransformerEmbeddings\n",
    "from langchain_chroma import Chroma\n",
    "CHROMA_PERSIST_DIRECTORY = \"./my_pdf_embeddings\"\n",
    "embeddings = SentenceTransformerEmbeddings(model_name=\"all-MiniLM-L6-v2\")\n",
    "documents_to_embed =chunks # Or full list of chunks for the entire PDF\n",
    "# This will now work AND store the embeddings to disk:\n",
    "\n",
    "if os.path.exists(CHROMA_PERSIST_DIRECTORY) and os.listdir(CHROMA_PERSIST_DIRECTORY):\n",
    "    print(f\"Loading existing vector store from: {CHROMA_PERSIST_DIRECTORY}\")\n",
    "    # Load the existing vector store from disk\n",
    "    db = Chroma(\n",
    "        persist_directory=CHROMA_PERSIST_DIRECTORY,\n",
    "        embedding_function=embeddings # providing embedding_function when loading\n",
    "    )\n",
    "else:\n",
    "    print(f\"Creating new vector store and saving to: {CHROMA_PERSIST_DIRECTORY}\")\n",
    "    # create  new Chroma vector store and persist it to disk\n",
    "    db = Chroma.from_documents(\n",
    "        documents=documents_to_embed, #  actual chunks from the PDF\n",
    "        embedding=embeddings,\n",
    "        persist_directory=CHROMA_PERSIST_DIRECTORY # THIS IS WHAT MAKES IT PERSISTENT\n",
    "    )\n",
    "    print(\"New vector store created and persisted.\")\n",
    "\n",
    "print(f\"Chroma DB created/loaded with {db._collection.count()} documents and persisted to {CHROMA_PERSIST_DIRECTORY}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b51a8c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-6.46311790e-02  7.81181753e-02 -8.63594562e-02 -2.75764782e-02\n",
      "  9.09177214e-03 -1.11269541e-01 -4.38521057e-02  3.23253199e-02\n",
      "  3.37249041e-02  1.83501393e-02 -1.85264554e-02  3.86686288e-02\n",
      "  6.75061718e-02 -7.84838721e-02 -1.49322627e-02  1.37332343e-02\n",
      " -2.38795597e-02 -5.21884523e-02  4.54042405e-02 -2.42471397e-02\n",
      "  1.10308386e-01 -7.80130699e-02 -1.90291442e-02 -1.33687258e-01\n",
      " -5.27529791e-03  1.42009649e-03  6.53517321e-02  1.06955506e-02\n",
      " -1.62832662e-02 -8.73600021e-02 -4.68272604e-02  2.68528913e-03\n",
      " -4.64460775e-02  4.86822315e-02  2.22515804e-03 -5.97296879e-02\n",
      " -1.79335158e-02 -3.63547318e-02 -1.72293447e-02 -4.45414856e-02\n",
      "  7.10950838e-03 -8.84814560e-02  3.35440002e-02  1.25445817e-02\n",
      " -2.65792403e-02  3.52119245e-02 -1.96016785e-02  1.81717351e-02\n",
      " -6.49815872e-02 -7.06792921e-02 -2.49345917e-02  5.28296269e-02\n",
      " -3.74432392e-02  7.76179880e-02 -7.80405104e-03  2.16678046e-02\n",
      "  7.48581365e-02  3.55439186e-02 -1.02373995e-02  4.36887331e-02\n",
      "  3.93901058e-02  2.76916418e-02  1.01949479e-02  1.42951291e-02\n",
      "  8.65597278e-03 -1.59853399e-02  7.89930150e-02 -4.89625894e-03\n",
      " -1.29600437e-02 -3.45458612e-02 -4.91267927e-02  1.43586257e-02\n",
      "  1.96430981e-02  6.75538629e-02  1.22324121e-03 -2.64552105e-02\n",
      " -4.59522083e-02  2.40643602e-02  2.66388105e-03 -9.56451148e-02\n",
      " -4.80778702e-02 -1.20219113e-02 -4.56237383e-02 -7.50216544e-02\n",
      "  2.88009556e-04  6.64480850e-02 -3.94937471e-02  1.95100065e-02\n",
      " -4.64535654e-02 -4.94695753e-02 -7.27721024e-03  7.10406750e-02\n",
      "  1.36241419e-02  7.18110651e-02 -6.46827295e-02 -7.33906105e-02\n",
      " -8.22248578e-04  1.68400090e-02 -7.83310365e-03  2.84698717e-02\n",
      "  5.44471294e-02  8.40073451e-02 -4.49321866e-02 -5.21067642e-02\n",
      " -3.51720722e-03 -2.63497196e-02  6.86319470e-02  1.82148796e-02\n",
      " -1.81016345e-02 -5.59451394e-02  2.78052897e-03  9.61712152e-02\n",
      " -9.08602998e-02 -4.71234918e-02 -1.35318739e-02  9.82194021e-03\n",
      " -7.59523511e-02  5.75542711e-02  1.69071350e-02 -2.60947067e-02\n",
      " -1.24771781e-02 -2.03656591e-02 -6.15830421e-02 -3.34817581e-02\n",
      " -6.47925362e-02 -1.14023171e-01  4.54356149e-02  1.23366486e-32\n",
      " -6.64106086e-02 -7.49610597e-03 -4.34286967e-02 -2.74566188e-02\n",
      "  8.73664487e-03  6.84950054e-02  3.76191735e-02  1.14282534e-01\n",
      " -2.72686034e-02  1.49205150e-02 -2.56584343e-02  2.61168312e-02\n",
      " -5.36237806e-02 -1.62282679e-02  7.20429346e-02 -3.78393121e-02\n",
      "  1.56217051e-04  2.26894226e-02  5.52460775e-02 -2.88044941e-03\n",
      "  8.07093829e-02 -6.42158687e-02  8.28475412e-03 -1.74285211e-02\n",
      "  3.89054455e-02  6.79770857e-02  5.37377084e-03 -4.90903594e-02\n",
      "  2.08422579e-02  4.91618812e-02  5.59155457e-02 -5.45308962e-02\n",
      " -1.96403097e-02  1.64023284e-02  3.36916521e-02  5.41466922e-02\n",
      " -5.73345423e-02 -1.05998544e-02 -6.54610479e-03 -4.47924435e-02\n",
      " -8.46428704e-03 -8.88416693e-02 -3.68885584e-02 -3.24696377e-02\n",
      " -1.01636671e-01 -3.41344178e-02  7.63381831e-03 -1.03138881e-02\n",
      "  1.64280869e-02  1.44645618e-02  4.26182942e-03 -3.89164686e-02\n",
      " -2.89216433e-02  2.15790030e-02  9.65358391e-02 -2.55897548e-02\n",
      "  4.94874753e-02  1.32694067e-02  5.38234152e-02  9.57624093e-02\n",
      " -6.99393898e-02  9.85513777e-02 -3.75239104e-02  8.47775675e-03\n",
      " -1.45479022e-02  4.00039926e-02  1.14625216e-01 -6.19685203e-02\n",
      "  8.01094808e-03 -6.77383170e-02  2.26813760e-02  1.11646764e-02\n",
      " -4.26086485e-02 -6.27589747e-02 -8.80113095e-02 -4.84278724e-02\n",
      " -1.40024908e-02  1.05997454e-02  5.92933111e-02  4.91691427e-03\n",
      " -5.81644922e-02 -3.22741307e-02 -7.20786536e-03 -3.88991646e-02\n",
      " -3.91691849e-02  3.01974230e-02 -3.56798954e-02 -1.72399431e-02\n",
      " -5.75361215e-02 -9.49645415e-03 -1.69213011e-03  2.03110860e-03\n",
      "  6.76064119e-02  1.58668105e-02  6.43541990e-03 -1.31365872e-32\n",
      " -3.81360501e-02  3.52229364e-02 -1.56979859e-02  5.41534163e-02\n",
      "  1.27742458e-02 -5.42790629e-02  6.71032220e-02  4.47926298e-02\n",
      " -1.19777967e-03  2.15586126e-02  4.92607104e-03 -6.15180880e-02\n",
      " -4.11906913e-02 -2.46294104e-02  6.22282773e-02 -1.53084509e-02\n",
      " -9.18895453e-02 -2.60620415e-02 -2.38291342e-02  1.48605527e-02\n",
      "  3.68798748e-02  1.39136747e-01  1.58345141e-03  1.13896104e-02\n",
      " -1.60499196e-02  3.50692011e-02 -1.17675336e-02  2.46251393e-02\n",
      "  3.18888202e-02  9.11913216e-02 -1.84663106e-03  1.51476292e-02\n",
      " -1.05225295e-01  1.03322454e-01  1.33449119e-02 -1.48946226e-01\n",
      "  1.15292944e-01  7.26928636e-02  3.33964676e-02  3.56930052e-03\n",
      "  5.50678633e-02  1.04028501e-01  1.44104790e-02  2.20846776e-02\n",
      " -4.38935831e-02 -7.19477907e-02 -1.89156309e-02  5.13440631e-02\n",
      "  3.85247655e-02 -5.07778078e-02 -7.49034900e-03 -5.89401498e-02\n",
      "  7.35997558e-02 -4.25530272e-03  1.11790076e-01  5.85116073e-02\n",
      "  2.35651825e-02  6.71011657e-02  3.56275886e-02  2.15742793e-02\n",
      "  9.69256386e-02 -1.21737778e-01  8.85666534e-02  4.84690815e-02\n",
      "  5.37244370e-03 -2.56544016e-02  1.02378979e-01  1.73209645e-02\n",
      " -1.81904249e-02  3.49007384e-03  6.89637884e-02  6.73903748e-02\n",
      "  4.75074872e-02 -5.93225583e-02 -1.86562315e-02 -8.25182498e-02\n",
      "  1.57035980e-02  4.35597897e-02  7.59991771e-03 -3.63457995e-03\n",
      " -1.41668422e-02  8.68628845e-02 -8.25594440e-02 -1.12672877e-02\n",
      " -3.30030248e-02 -9.75131765e-02  4.09466680e-03 -1.85773242e-02\n",
      "  3.80532220e-02 -1.75602194e-02 -9.79531929e-02  8.14454779e-02\n",
      " -4.53796331e-03  6.08192496e-02 -1.00954100e-01 -7.63351764e-08\n",
      " -9.75055397e-02 -7.43849063e-03 -1.42678078e-02  1.84211367e-05\n",
      "  3.02688535e-02  1.44489538e-02 -1.35215847e-02 -1.23037584e-01\n",
      "  2.96982992e-02 -1.52452141e-01  8.72849599e-02 -1.22336410e-01\n",
      " -3.34600769e-02 -1.37531767e-02  6.14357600e-03 -3.23335156e-02\n",
      " -2.84176948e-03 -2.86588445e-02  2.26079905e-03  8.17925408e-02\n",
      "  1.12811979e-02 -3.70712578e-02  3.89272869e-02  7.62635656e-03\n",
      "  4.88761924e-02  2.69851219e-02 -2.93300077e-02  5.29089272e-02\n",
      "  3.70715484e-02  3.84966051e-03 -7.63105974e-02  7.59712979e-02\n",
      "  2.26735957e-02  7.15830550e-02  3.97321116e-03  6.25060946e-02\n",
      " -2.06620097e-02  1.17261484e-02 -3.02231126e-02 -4.35915496e-03\n",
      "  1.23733487e-02 -4.07340229e-02 -1.48605388e-02  3.28110904e-02\n",
      "  3.59033085e-02 -6.86902031e-02  3.22279483e-02 -3.56899127e-02\n",
      " -4.30986062e-02  6.21488877e-02 -9.32203084e-02 -6.06774352e-02\n",
      " -5.61522022e-02  9.14309639e-03 -6.36552786e-03  1.56874824e-02\n",
      "  2.63216527e-04 -2.55904277e-03  3.05035189e-02  6.12767674e-02\n",
      "  4.32232507e-02  1.70824770e-02 -8.42918828e-03  5.38522452e-02]\n",
      "(384,)\n"
     ]
    }
   ],
   "source": [
    "import  numpy as np\n",
    "print(np.array(embeddings.embed_query(chunks[0].page_content)))\n",
    "print(np.array(embeddings.embed_query(chunks[0].page_content)).shape)\n",
    "# every piece of text embeded ( document chunks, and later the query)\n",
    "# is transformed into a point in a 384-dimensional space.That single dimension has a length of 384.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a61ff54d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Information and Communication Technology (ICT)] (IITP-2025-RS-2021-II211835).\\nABSTRACT Recently, legal practice has seen a significant rise in the adoption of Artificial Intelligence\\n(AI) for various core tasks. However, these technologies remain in their early stages and face challenges\\nsuch as understanding complex legal reasoning, managing biased data, ensuring transparency, and avoiding\\nmisleading responses, commonly referred to as hallucinations. To address these limitations, this paper'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# vector database\n",
    "query=\"\"\"Recently, legal practice has seen a significant rise in the adoption of Artificial Intelligence\n",
    "(AI) for various core tasks.\"\"\"\n",
    "result=db.similarity_search(query)\n",
    "result[0].page_content\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c41b0d3d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': 'speech.txt'}, page_content='The world must be made safe for democracy. Its peace must be planted upon the tested foundations of political liberty. We have no selfish ends to serve. We desire no conquest, no dominion. We seek no indemnities for ourselves, no material compensation for the sacrifices we shall freely make. We are but one of the champions of the rights of mankind. We shall be satisfied when those rights have been made as secure as the faith and the freedom of nations can make them.\\n\\nJust because we fight without rancor and without selfish object, seeking nothing for ourselves but what we shall wish to share with all free peoples, we shall, I feel confident, conduct our operations as belligerents without passion and ourselves observe with proud punctilio the principles of right and of fair play we profess to be fighting for.\\n\\nâ€¦\\n\\nIt will be all the easier for us to conduct ourselves as belligerents in a high spirit of right and fairness because we act without animus, not in enmity toward a people or with the desire to bring any injury or disadvantage upon them, but only in armed opposition to an irresponsible government which has thrown aside all considerations of humanity and of right and is running amuck. We are, let me say again, the sincere friends of the German people, and shall desire nothing so much as the early reestablishment of intimate relations of mutual advantage between usâ€”however hard it may be for them, for the time being, to believe that this is spoken from our hearts.\\n\\nWe have borne with their present government through all these bitter months because of that friendshipâ€”exercising a patience and forbearance which would otherwise have been impossible. We shall, happily, still have an opportunity to prove that friendship in our daily attitude and actions toward the millions of men and women of German birth and native sympathy who live among us and share our life, and we shall be proud to prove it toward all who are in fact loyal to their neighbors and to the government in the hour of test. They are, most of them, as true and loyal Americans as if they had never known any other fealty or allegiance. They will be prompt to stand with us in rebuking and restraining the few who may be of a different mind and purpose. If there should be disloyalty, it will be dealt with with a firm hand of stern repression; but, if it lifts its head at all, it will lift it only here and there and without countenance except from a lawless and malignant few.\\n\\nIt is a distressing and oppressive duty, gentlemen of the Congress, which I have performed in thus addressing you. There are, it may be, many months of fiery trial and sacrifice ahead of us. It is a fearful thing to lead this great peaceful people into war, into the most terrible and disastrous of all wars, civilization itself seeming to be in the balance. But the right is more precious than peace, and we shall fight for the things which we have always carried nearest our heartsâ€”for democracy, for the right of those who submit to authority to have a voice in their own governments, for the rights and liberties of small nations, for a universal dominion of right by such a concert of free peoples as shall bring peace and safety to all nations and make the world itself at last free.\\n\\nTo such a task we can dedicate our lives and our fortunes, everything that we are and everything that we have, with the pride of those who know that the day has come when America is privileged to spend her blood and her might for the principles that gave her birth and happiness and the peace which she has treasured. God helping her, she can do no other.')]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# data ingestion\n",
    "from langchain_community.document_loaders import TextLoader\n",
    "loader=TextLoader(\"speech.txt\")\n",
    "text_documents=loader.load()\n",
    "text_documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8266617",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LangSmith tracing enabled if environment variables are set correctly.\n",
      "Tracing enabled: true\n",
      "LangSmith Project: ravogate\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "\n",
    "print(\"LangSmith tracing enabled if environment variables are set correctly.\")\n",
    "print(f\"Tracing enabled: {os.environ.get('LANGCHAIN_TRACING_V2')}\")\n",
    "print(f\"LangSmith Project: {os.environ.get('LANGCHAIN_PROJECT')}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49667b1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pdf reader\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "loader=PyPDFLoader(\"Legal_Query_RAG_IEEE.pdf\")\n",
    "docs=loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a3c3787",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'producer': 'pdfTeX-1.40.24; modified using iText® Core 7.2.4 (AGPL version) ©2000-2022 iText Group NV', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-02-27T19:55:58+05:30', 'moddate': '2025-02-28T17:25:14-05:00', 'ieee article id': '10887211', 'trapped': 'False', 'ieee issue id': '10820123', 'subject': 'IEEE Access;2025;13; ;10.1109/ACCESS.2025.3542125', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.24 (TeX Live 2022) kpathsea version 6.3.4', 'ieee publication id': '6287639', 'title': 'Legal Query RAG', 'source': 'Legal_Query_RAG_IEEE.pdf', 'total_pages': 17, 'page': 0, 'page_label': '36978'}, page_content='Received 14 January 2025, accepted 31 January 2025, date of publication 14 February 2025, date of current version 3 March 2025.\\nDigital Object Identifier 10.1 109/ACCESS.2025.3542125\\nLegal Query RAG\\nRAHMAN S. M. WAHIDUR\\n1, SUMIN KIM\\n 2, HAEUNG CHOI\\n 1, DAVID S. BHATTI\\n1,\\nAND HEUNG-NO LEE\\n 1, (Senior Member, IEEE)\\n1School of Electrical Engineering and Computer Science, Gwangju Institute of Science and Technology, Gwangju 61005, South Korea'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.24; modified using iText® Core 7.2.4 (AGPL version) ©2000-2022 iText Group NV', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-02-27T19:55:58+05:30', 'moddate': '2025-02-28T17:25:14-05:00', 'ieee article id': '10887211', 'trapped': 'False', 'ieee issue id': '10820123', 'subject': 'IEEE Access;2025;13; ;10.1109/ACCESS.2025.3542125', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.24 (TeX Live 2022) kpathsea version 6.3.4', 'ieee publication id': '6287639', 'title': 'Legal Query RAG', 'source': 'Legal_Query_RAG_IEEE.pdf', 'total_pages': 17, 'page': 0, 'page_label': '36978'}, page_content='1, DAVID S. BHATTI\\n1,\\nAND HEUNG-NO LEE\\n 1, (Senior Member, IEEE)\\n1School of Electrical Engineering and Computer Science, Gwangju Institute of Science and Technology, Gwangju 61005, South Korea\\n2Artificial Intelligence Graduate School, Gwangju Institute of Science and Technology, Gwangju 61005, South Korea\\nCorresponding author: Heung-No Lee (heungno@gist.ac.kr)\\nThis work was supported by Institute of Information & communications Technology Planning & Evaluation (IITP) grant funded by the')]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "text_splitter=RecursiveCharacterTextSplitter(chunk_size=500,chunk_overlap=200)\n",
    "chunks=text_splitter.split_documents(docs)\n",
    "documents=text_splitter.split_documents(docs)\n",
    "documents[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65369ad6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading existing vector store from: ./my_pdf_embeddings\n",
      "Chroma DB created/loaded with 227 documents and persisted to ./my_pdf_embeddings\n"
     ]
    }
   ],
   "source": [
    "# vectorization embedding and vector store\n",
    "from langchain_community.embeddings import SentenceTransformerEmbeddings\n",
    "from langchain_chroma import Chroma\n",
    "CHROMA_PERSIST_DIRECTORY = \"./my_pdf_embeddings\"\n",
    "embeddings = SentenceTransformerEmbeddings(model_name=\"all-MiniLM-L6-v2\")\n",
    "documents_to_embed =chunks # Or full list of chunks for the entire PDF\n",
    "# This will now work AND store the embeddings to disk:\n",
    "\n",
    "if os.path.exists(CHROMA_PERSIST_DIRECTORY) and os.listdir(CHROMA_PERSIST_DIRECTORY):\n",
    "    print(f\"Loading existing vector store from: {CHROMA_PERSIST_DIRECTORY}\")\n",
    "    # Load the existing vector store from disk\n",
    "    db = Chroma(\n",
    "        persist_directory=CHROMA_PERSIST_DIRECTORY,\n",
    "        embedding_function=embeddings # providing embedding_function when loading\n",
    "    )\n",
    "else:\n",
    "    print(f\"Creating new vector store and saving to: {CHROMA_PERSIST_DIRECTORY}\")\n",
    "    # Create a new Chroma vector store and persist it to disk\n",
    "    db = Chroma.from_documents(\n",
    "        documents=documents_to_embed, #  actual chunks from the PDF\n",
    "        embedding=embeddings,\n",
    "        persist_directory=CHROMA_PERSIST_DIRECTORY # THIS IS WHAT MAKES IT PERSISTENT\n",
    "    )\n",
    "    print(\"New vector store created and persisted.\")\n",
    "\n",
    "print(f\"Chroma DB created/loaded with {db._collection.count()} documents and persisted to {CHROMA_PERSIST_DIRECTORY}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdfebf4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-6.46311790e-02  7.81181753e-02 -8.63594562e-02 -2.75764782e-02\n",
      "  9.09177214e-03 -1.11269541e-01 -4.38521057e-02  3.23253199e-02\n",
      "  3.37249041e-02  1.83501393e-02 -1.85264554e-02  3.86686288e-02\n",
      "  6.75061718e-02 -7.84838721e-02 -1.49322627e-02  1.37332343e-02\n",
      " -2.38795597e-02 -5.21884523e-02  4.54042405e-02 -2.42471397e-02\n",
      "  1.10308386e-01 -7.80130699e-02 -1.90291442e-02 -1.33687258e-01\n",
      " -5.27529791e-03  1.42009649e-03  6.53517321e-02  1.06955506e-02\n",
      " -1.62832662e-02 -8.73600021e-02 -4.68272604e-02  2.68528913e-03\n",
      " -4.64460775e-02  4.86822315e-02  2.22515804e-03 -5.97296879e-02\n",
      " -1.79335158e-02 -3.63547318e-02 -1.72293447e-02 -4.45414856e-02\n",
      "  7.10950838e-03 -8.84814560e-02  3.35440002e-02  1.25445817e-02\n",
      " -2.65792403e-02  3.52119245e-02 -1.96016785e-02  1.81717351e-02\n",
      " -6.49815872e-02 -7.06792921e-02 -2.49345917e-02  5.28296269e-02\n",
      " -3.74432392e-02  7.76179880e-02 -7.80405104e-03  2.16678046e-02\n",
      "  7.48581365e-02  3.55439186e-02 -1.02373995e-02  4.36887331e-02\n",
      "  3.93901058e-02  2.76916418e-02  1.01949479e-02  1.42951291e-02\n",
      "  8.65597278e-03 -1.59853399e-02  7.89930150e-02 -4.89625894e-03\n",
      " -1.29600437e-02 -3.45458612e-02 -4.91267927e-02  1.43586257e-02\n",
      "  1.96430981e-02  6.75538629e-02  1.22324121e-03 -2.64552105e-02\n",
      " -4.59522083e-02  2.40643602e-02  2.66388105e-03 -9.56451148e-02\n",
      " -4.80778702e-02 -1.20219113e-02 -4.56237383e-02 -7.50216544e-02\n",
      "  2.88009556e-04  6.64480850e-02 -3.94937471e-02  1.95100065e-02\n",
      " -4.64535654e-02 -4.94695753e-02 -7.27721024e-03  7.10406750e-02\n",
      "  1.36241419e-02  7.18110651e-02 -6.46827295e-02 -7.33906105e-02\n",
      " -8.22248578e-04  1.68400090e-02 -7.83310365e-03  2.84698717e-02\n",
      "  5.44471294e-02  8.40073451e-02 -4.49321866e-02 -5.21067642e-02\n",
      " -3.51720722e-03 -2.63497196e-02  6.86319470e-02  1.82148796e-02\n",
      " -1.81016345e-02 -5.59451394e-02  2.78052897e-03  9.61712152e-02\n",
      " -9.08602998e-02 -4.71234918e-02 -1.35318739e-02  9.82194021e-03\n",
      " -7.59523511e-02  5.75542711e-02  1.69071350e-02 -2.60947067e-02\n",
      " -1.24771781e-02 -2.03656591e-02 -6.15830421e-02 -3.34817581e-02\n",
      " -6.47925362e-02 -1.14023171e-01  4.54356149e-02  1.23366486e-32\n",
      " -6.64106086e-02 -7.49610597e-03 -4.34286967e-02 -2.74566188e-02\n",
      "  8.73664487e-03  6.84950054e-02  3.76191735e-02  1.14282534e-01\n",
      " -2.72686034e-02  1.49205150e-02 -2.56584343e-02  2.61168312e-02\n",
      " -5.36237806e-02 -1.62282679e-02  7.20429346e-02 -3.78393121e-02\n",
      "  1.56217051e-04  2.26894226e-02  5.52460775e-02 -2.88044941e-03\n",
      "  8.07093829e-02 -6.42158687e-02  8.28475412e-03 -1.74285211e-02\n",
      "  3.89054455e-02  6.79770857e-02  5.37377084e-03 -4.90903594e-02\n",
      "  2.08422579e-02  4.91618812e-02  5.59155457e-02 -5.45308962e-02\n",
      " -1.96403097e-02  1.64023284e-02  3.36916521e-02  5.41466922e-02\n",
      " -5.73345423e-02 -1.05998544e-02 -6.54610479e-03 -4.47924435e-02\n",
      " -8.46428704e-03 -8.88416693e-02 -3.68885584e-02 -3.24696377e-02\n",
      " -1.01636671e-01 -3.41344178e-02  7.63381831e-03 -1.03138881e-02\n",
      "  1.64280869e-02  1.44645618e-02  4.26182942e-03 -3.89164686e-02\n",
      " -2.89216433e-02  2.15790030e-02  9.65358391e-02 -2.55897548e-02\n",
      "  4.94874753e-02  1.32694067e-02  5.38234152e-02  9.57624093e-02\n",
      " -6.99393898e-02  9.85513777e-02 -3.75239104e-02  8.47775675e-03\n",
      " -1.45479022e-02  4.00039926e-02  1.14625216e-01 -6.19685203e-02\n",
      "  8.01094808e-03 -6.77383170e-02  2.26813760e-02  1.11646764e-02\n",
      " -4.26086485e-02 -6.27589747e-02 -8.80113095e-02 -4.84278724e-02\n",
      " -1.40024908e-02  1.05997454e-02  5.92933111e-02  4.91691427e-03\n",
      " -5.81644922e-02 -3.22741307e-02 -7.20786536e-03 -3.88991646e-02\n",
      " -3.91691849e-02  3.01974230e-02 -3.56798954e-02 -1.72399431e-02\n",
      " -5.75361215e-02 -9.49645415e-03 -1.69213011e-03  2.03110860e-03\n",
      "  6.76064119e-02  1.58668105e-02  6.43541990e-03 -1.31365872e-32\n",
      " -3.81360501e-02  3.52229364e-02 -1.56979859e-02  5.41534163e-02\n",
      "  1.27742458e-02 -5.42790629e-02  6.71032220e-02  4.47926298e-02\n",
      " -1.19777967e-03  2.15586126e-02  4.92607104e-03 -6.15180880e-02\n",
      " -4.11906913e-02 -2.46294104e-02  6.22282773e-02 -1.53084509e-02\n",
      " -9.18895453e-02 -2.60620415e-02 -2.38291342e-02  1.48605527e-02\n",
      "  3.68798748e-02  1.39136747e-01  1.58345141e-03  1.13896104e-02\n",
      " -1.60499196e-02  3.50692011e-02 -1.17675336e-02  2.46251393e-02\n",
      "  3.18888202e-02  9.11913216e-02 -1.84663106e-03  1.51476292e-02\n",
      " -1.05225295e-01  1.03322454e-01  1.33449119e-02 -1.48946226e-01\n",
      "  1.15292944e-01  7.26928636e-02  3.33964676e-02  3.56930052e-03\n",
      "  5.50678633e-02  1.04028501e-01  1.44104790e-02  2.20846776e-02\n",
      " -4.38935831e-02 -7.19477907e-02 -1.89156309e-02  5.13440631e-02\n",
      "  3.85247655e-02 -5.07778078e-02 -7.49034900e-03 -5.89401498e-02\n",
      "  7.35997558e-02 -4.25530272e-03  1.11790076e-01  5.85116073e-02\n",
      "  2.35651825e-02  6.71011657e-02  3.56275886e-02  2.15742793e-02\n",
      "  9.69256386e-02 -1.21737778e-01  8.85666534e-02  4.84690815e-02\n",
      "  5.37244370e-03 -2.56544016e-02  1.02378979e-01  1.73209645e-02\n",
      " -1.81904249e-02  3.49007384e-03  6.89637884e-02  6.73903748e-02\n",
      "  4.75074872e-02 -5.93225583e-02 -1.86562315e-02 -8.25182498e-02\n",
      "  1.57035980e-02  4.35597897e-02  7.59991771e-03 -3.63457995e-03\n",
      " -1.41668422e-02  8.68628845e-02 -8.25594440e-02 -1.12672877e-02\n",
      " -3.30030248e-02 -9.75131765e-02  4.09466680e-03 -1.85773242e-02\n",
      "  3.80532220e-02 -1.75602194e-02 -9.79531929e-02  8.14454779e-02\n",
      " -4.53796331e-03  6.08192496e-02 -1.00954100e-01 -7.63351764e-08\n",
      " -9.75055397e-02 -7.43849063e-03 -1.42678078e-02  1.84211367e-05\n",
      "  3.02688535e-02  1.44489538e-02 -1.35215847e-02 -1.23037584e-01\n",
      "  2.96982992e-02 -1.52452141e-01  8.72849599e-02 -1.22336410e-01\n",
      " -3.34600769e-02 -1.37531767e-02  6.14357600e-03 -3.23335156e-02\n",
      " -2.84176948e-03 -2.86588445e-02  2.26079905e-03  8.17925408e-02\n",
      "  1.12811979e-02 -3.70712578e-02  3.89272869e-02  7.62635656e-03\n",
      "  4.88761924e-02  2.69851219e-02 -2.93300077e-02  5.29089272e-02\n",
      "  3.70715484e-02  3.84966051e-03 -7.63105974e-02  7.59712979e-02\n",
      "  2.26735957e-02  7.15830550e-02  3.97321116e-03  6.25060946e-02\n",
      " -2.06620097e-02  1.17261484e-02 -3.02231126e-02 -4.35915496e-03\n",
      "  1.23733487e-02 -4.07340229e-02 -1.48605388e-02  3.28110904e-02\n",
      "  3.59033085e-02 -6.86902031e-02  3.22279483e-02 -3.56899127e-02\n",
      " -4.30986062e-02  6.21488877e-02 -9.32203084e-02 -6.06774352e-02\n",
      " -5.61522022e-02  9.14309639e-03 -6.36552786e-03  1.56874824e-02\n",
      "  2.63216527e-04 -2.55904277e-03  3.05035189e-02  6.12767674e-02\n",
      "  4.32232507e-02  1.70824770e-02 -8.42918828e-03  5.38522452e-02]\n",
      "(384,)\n"
     ]
    }
   ],
   "source": [
    "import  numpy as np\n",
    "print(np.array(embeddings.embed_query(chunks[0].page_content)))\n",
    "print(np.array(embeddings.embed_query(chunks[0].page_content)).shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dad63211",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Information and Communication Technology (ICT)] (IITP-2025-RS-2021-II211835).\\nABSTRACT Recently, legal practice has seen a significant rise in the adoption of Artificial Intelligence\\n(AI) for various core tasks. However, these technologies remain in their early stages and face challenges\\nsuch as understanding complex legal reasoning, managing biased data, ensuring transparency, and avoiding\\nmisleading responses, commonly referred to as hallucinations. To address these limitations, this paper'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# vector database\n",
    "query=\"\"\"Recently, legal practice has seen a significant rise in the adoption of Artificial Intelligence\n",
    "(AI) for various core tasks.\"\"\"\n",
    "result=db.similarity_search(query)\n",
    "result[0].page_content\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89779057",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ollama LLM Tinyllama initialized\n"
     ]
    }
   ],
   "source": [
    "from langchain_ollama import OllamaLLM\n",
    "from langchain_core.prompts import ChatPromptTemplate #chat based prompts\n",
    "from langchain_core.output_parsers import StrOutputParser #raw to usable format out\n",
    "from langchain.chains.combine_documents import create_stuff_documents_chain \n",
    "#multiple chunks into single usable input \n",
    "from langchain.chains import create_retrieval_chain #to orchestrate entire rag pipeline\n",
    "\n",
    "from langchain.chains import ConversationalRetrievalChain\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain_core.messages import HumanMessage, AIMessage\n",
    "llm=OllamaLLM(model=\"tinyllama\")\n",
    "print(\"Ollama LLM Tinyllama initialized\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b37a1bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RAG PROMPT TEMPLATE DEFINED.\n"
     ]
    }
   ],
   "source": [
    "# --- RAG Prompt for Answer Generation ---\n",
    "# This prompt is specifically for how the LLM should answer *after* relevant context is retrieved.\n",
    "rag_prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"You are a helpful AI assistant. Use only the following provided context to answer the question. If the answer is not in the context, clearly state 'I don't know' or 'Answer not in the provided document'.\\n\\nContext:\\n{context}\"),\n",
    "    (\"user\", \"{question}\") # <-- CHANGED: From '{input}' to '{question}'\n",
    "])\n",
    "print(\"RAG PROMPT TEMPLATE DEFINED.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b0281be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retriever created from Chroma DB.\n"
     ]
    }
   ],
   "source": [
    "# retriever from vector store and return top k matching results\n",
    "# This object is what the retrieval chain will use to perform similarity searches\n",
    "retriever = db.as_retriever() \n",
    "#method to transform vector store to retreiver\n",
    "print(\"Retriever created from Chroma DB.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1ae1101",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- AI Assistant (Conversational PDF Search): Type 'exit' or 'quit' to end the conversation. ---\n",
      "Hello! How can I help you with your legal document today?\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new StuffDocumentsChain chain...\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mSystem: You are a helpful AI assistant. Use only the following provided context to answer the question. If the answer is not in the context, clearly state 'I don't know' or 'Answer not in the provided document'.\n",
      "\n",
      "Context:\n",
      "adaptation, and improved adherence to instructions.\n",
      "4) Extensive evaluations were performed to assess the\n",
      "performance of the proposed RAG system. The results\n",
      "demonstrate that LQ-RAG consistently outperforms\n",
      "baseline models, highlighting its applicability in the\n",
      "legal domain.\n",
      "The subsequent sections of this paper are structured\n",
      "as follows. Section II provides background information.\n",
      "Section III reviews pertinent literature. Section IV unveils\n",
      "\n",
      "LQ-RAG is depicted in Algorithm 3.\n",
      "Algorithm 1Embedding LLM Fine-Tuning Process\n",
      "Constants: Loss function MNRL, Evaluator Eval,\n",
      "Learning Rate η\n",
      "Input: Csub-legal\n",
      "Output: Trained LLM network parameters θglobal\n",
      "1: Data Collection and Preprocessing:\n",
      "2: Dsynthetic ← LLM(Csub-legal)\n",
      "3: Dsynthetic ∈ {Dtrain, Deval}\n",
      "4: Initialize & Fine-Tune Embedding Model:\n",
      "5: Baseline-Model M ← Pre-trained embedding LLM\n",
      "6: Ltrain_embed ← DataLoader(Dtrain, batch_size)\n",
      "7: for each epochs do\n",
      "\n",
      "proposed LQ-RAG system. The proposed system is organized\n",
      "into two primary parts: Fine-Tuning (FT) Layer and RAG\n",
      "Layer. The FT Layer involves fine-tuning both the embedding\n",
      "LLM and the generative LLM. In contrast, the RAG Layer\n",
      "integrates advanced RAG modules, an evaluation agent,\n",
      "a prompt engineering agent, and a feedback mechanism to\n",
      "ensure the quality and accuracy of the generated responses.\n",
      "The bottom left quadrant of the FT Layer illustrates\n",
      "\n",
      "of 80%, showing a 23% improvement over Naive RAG and\n",
      "a 14% improvement over RAG with FTM. This substantial\n",
      "improvement is attributed to the advanced integration and\n",
      "fine-tuning techniques in LQ-RAG, which enhance its ability\n",
      "to understand and retrieve contextually relevant information,\n",
      "resulting in more coherent answers. The evaluation results\n",
      "as illustrated in Figure 8, further reinforce the effectiveness\n",
      "of the proposed system. For the same evaluation dataset, the\n",
      "Human: tell me about lq rag\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "AI Assistant: Context:\n",
      "1) Adaptation, and improved adherence to instructional instructions.\n",
      "2) A thorough review of relevant literature was conducted.\n",
      "3) The proposed RAG system was organized into two primary parts: fine-tuning (FT), which involves both embedding LLM and generating AA agents, and prompt engineering (PE), which is an evaluation agent and feedback mechanism.\n",
      "4) Experimental results demonstrate substantial improvements in FT Layer with advanced integration and fine-tuning techniques over baseline models, indicating the effectiveness of RAG system in retrieving contextually relevant information, resulting in more coherent answers.\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mGiven the following conversation and a follow up question, rephrase the follow up question to be a standalone question, in its original language.\n",
      "\n",
      "Chat History:\n",
      "\n",
      "Human: tell me about lq rag\n",
      "Assistant: Context:\n",
      "1) Adaptation, and improved adherence to instructional instructions.\n",
      "2) A thorough review of relevant literature was conducted.\n",
      "3) The proposed RAG system was organized into two primary parts: fine-tuning (FT), which involves both embedding LLM and generating AA agents, and prompt engineering (PE), which is an evaluation agent and feedback mechanism.\n",
      "4) Experimental results demonstrate substantial improvements in FT Layer with advanced integration and fine-tuning techniques over baseline models, indicating the effectiveness of RAG system in retrieving contextually relevant information, resulting in more coherent answers.\n",
      "Follow Up Input: what was my previous question?\n",
      "Standalone question:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new StuffDocumentsChain chain...\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mSystem: You are a helpful AI assistant. Use only the following provided context to answer the question. If the answer is not in the context, clearly state 'I don't know' or 'Answer not in the provided document'.\n",
      "\n",
      "Context:\n",
      "adaptation, and improved adherence to instructions.\n",
      "4) Extensive evaluations were performed to assess the\n",
      "performance of the proposed RAG system. The results\n",
      "demonstrate that LQ-RAG consistently outperforms\n",
      "baseline models, highlighting its applicability in the\n",
      "legal domain.\n",
      "The subsequent sections of this paper are structured\n",
      "as follows. Section II provides background information.\n",
      "Section III reviews pertinent literature. Section IV unveils\n",
      "\n",
      "is double the time of the Naive case. From this, it can be\n",
      "inferred that while the Naive RAG system responds faster,\n",
      "incorporating advanced modules increases the system’s time\n",
      "complexity.\n",
      "VII. CONCLUSION\n",
      "This paper addresses domain-specific challenges in the legal\n",
      "field, where traditional RAG systems often fail in information\n",
      "extraction and response generation. To address these issues,\n",
      "the LQ-RAG framework integrates RAG with a recursive\n",
      "feedback mechanism, combining specialized LLMs and an\n",
      "\n",
      "of 80%, showing a 23% improvement over Naive RAG and\n",
      "a 14% improvement over RAG with FTM. This substantial\n",
      "improvement is attributed to the advanced integration and\n",
      "fine-tuning techniques in LQ-RAG, which enhance its ability\n",
      "to understand and retrieve contextually relevant information,\n",
      "resulting in more coherent answers. The evaluation results\n",
      "as illustrated in Figure 8, further reinforce the effectiveness\n",
      "of the proposed system. For the same evaluation dataset, the\n",
      "\n",
      "R. S. M. Wahidur et al.: Legal Query RAG\n",
      "FIGURE 1. The schematic diagram of the proposed legal query RAG. The diagram is divided into two main components: Fine-tuning (FT) layer and RAG\n",
      "layer. The FT layer focuses on fine-tuning processes of embedding LLM and generative LLM. On the other hand, the RAG layer incorporates different RAG\n",
      "modules with fine-tuned LLMs, an evaluation agent, and a feedback system designed to enhance the accuracy and quality of the generated responses.\n",
      "Human: Sure! The follow-up question is \"Can you provide more details about the fine-tuining process and how it impacts the proposed RAG system's adoption in retrieving contextually relevant information?\"\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "AI Assistant: Yes, I can provide more details on the fine-tuining process and how it contributes to the proposed RAG system's adoption in retrieving contextually relevant information.\n",
      "\n",
      "The FT layer of the proposed legal query RAG (LQ-RAG) is composed of several modules that are designed to fine-tune the embedded language model (LLM) and generate more accurate and diverse responses. The LLM module, which is responsible for generating textual embeddings, is first trained with a set of predefined input-output pairs in order to learn the underlying semantics of the given domain. This process involves a combination of image captioning and language modeling tasks to improve the LLM's ability to generate natural language responses.\n",
      "\n",
      "Once trained, the fine-tuining process is performed on each retrieved piece of information (e.g., a document, legal text, or court case) by the RAG layer. The RAG module learns to generate responses that are contextually relevant to the given information based on the output embeddings generated from the FT layer. The fine-tuining process involves several tasks, such as predicting the next word in a sentence or selecting the most likely response based on the contextual relevance of its preceding elements.\n",
      "\n",
      "The RAG module also incorporates an evaluation agent that analyzes and refines the generated responses through feedback. This feedback is provided by human annotators who provide suggestions for improving the RAG's performance, such as modifying the fine-tuining process or adding new modules to enhance the overall system's efficiency.\n",
      "\n",
      "In summary, LQ-RAG's proposed fine-tuining process enables it to generate contextually relevant and accurate responses from retrieved information, thereby improving its adoption in retrieving contextually relevant information.\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mGiven the following conversation and a follow up question, rephrase the follow up question to be a standalone question, in its original language.\n",
      "\n",
      "Chat History:\n",
      "\n",
      "Human: tell me about lq rag\n",
      "Assistant: Context:\n",
      "1) Adaptation, and improved adherence to instructional instructions.\n",
      "2) A thorough review of relevant literature was conducted.\n",
      "3) The proposed RAG system was organized into two primary parts: fine-tuning (FT), which involves both embedding LLM and generating AA agents, and prompt engineering (PE), which is an evaluation agent and feedback mechanism.\n",
      "4) Experimental results demonstrate substantial improvements in FT Layer with advanced integration and fine-tuning techniques over baseline models, indicating the effectiveness of RAG system in retrieving contextually relevant information, resulting in more coherent answers.\n",
      "Human: what was my previous question?\n",
      "Assistant: Yes, I can provide more details on the fine-tuining process and how it contributes to the proposed RAG system's adoption in retrieving contextually relevant information.\n",
      "\n",
      "The FT layer of the proposed legal query RAG (LQ-RAG) is composed of several modules that are designed to fine-tune the embedded language model (LLM) and generate more accurate and diverse responses. The LLM module, which is responsible for generating textual embeddings, is first trained with a set of predefined input-output pairs in order to learn the underlying semantics of the given domain. This process involves a combination of image captioning and language modeling tasks to improve the LLM's ability to generate natural language responses.\n",
      "\n",
      "Once trained, the fine-tuining process is performed on each retrieved piece of information (e.g., a document, legal text, or court case) by the RAG layer. The RAG module learns to generate responses that are contextually relevant to the given information based on the output embeddings generated from the FT layer. The fine-tuining process involves several tasks, such as predicting the next word in a sentence or selecting the most likely response based on the contextual relevance of its preceding elements.\n",
      "\n",
      "The RAG module also incorporates an evaluation agent that analyzes and refines the generated responses through feedback. This feedback is provided by human annotators who provide suggestions for improving the RAG's performance, such as modifying the fine-tuining process or adding new modules to enhance the overall system's efficiency.\n",
      "\n",
      "In summary, LQ-RAG's proposed fine-tuining process enables it to generate contextually relevant and accurate responses from retrieved information, thereby improving its adoption in retrieving contextually relevant information.\n",
      "Follow Up Input: what question i have asked you >\n",
      "Standalone question:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new StuffDocumentsChain chain...\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mSystem: You are a helpful AI assistant. Use only the following provided context to answer the question. If the answer is not in the context, clearly state 'I don't know' or 'Answer not in the provided document'.\n",
      "\n",
      "Context:\n",
      "R. S. M. Wahidur et al.: Legal Query RAG\n",
      "TABLE 10. Sample questions for evaluating the performance of LQ-RAG system.\n",
      "TABLE 11. Performance evaluation of the LQ-RAG system across different user queries.\n",
      "setups. This outcome was anticipated as the system was not\n",
      "provided with any relevant context information during the\n",
      "response generation. Based on these findings, the answers\n",
      "generated by the RAG systems are potentially incorrect\n",
      "because the retrieved information lacks the required context.\n",
      "\n",
      "R. S. M. Wahidur et al.: Legal Query RAG\n",
      "FIGURE 1. The schematic diagram of the proposed legal query RAG. The diagram is divided into two main components: Fine-tuning (FT) layer and RAG\n",
      "layer. The FT layer focuses on fine-tuning processes of embedding LLM and generative LLM. On the other hand, the RAG layer incorporates different RAG\n",
      "modules with fine-tuned LLMs, an evaluation agent, and a feedback system designed to enhance the accuracy and quality of the generated responses.\n",
      "\n",
      "extraction and response generation. To address these issues,\n",
      "the LQ-RAG framework integrates RAG with a recursive\n",
      "feedback mechanism, combining specialized LLMs and an\n",
      "agent-driven approach for response evaluation and query\n",
      "engineering. This multi-layered system reduces hallucina-\n",
      "tions and ensures precise, contextually relevant responses.\n",
      "Fine-tuning a general-purpose LLM with legal corpora\n",
      "resulted in a 15% improvement over baseline models, while\n",
      "\n",
      "misleading responses, commonly referred to as hallucinations. To address these limitations, this paper\n",
      "introduces Legal Query RAG (LQ-RAG), a novel Retrieval-Augmented Generation framework with a\n",
      "recursive feedback mechanism specifically designed to overcome the critical shortcomings of standard RAG\n",
      "implementations in legal applications. The proposed framework incorporates four key components: a custom\n",
      "Human: Yes, here is a follow-up question: How does the fine-tuining process work in the proposed legal query RAG (LQ-RAG) to generate more accurate and diverse responses from retrieved information?\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "AI Assistant: The provided context only addresses the Legal Query RAG system as a whole, not the specific performance evaluation of different user queries. In response generation, the fine-tuining process works by combining specialized LLMs with an agent-driven approach for response evaluation and query engineering. However, to address the limitations mentioned in the context, the LQ-RAG framework integrates a recursive feedback mechanism to overcome the critical shortcoming of standard RAG implementations in legal applications. The proposed system aims to generate more accurate and diverse responses from retrieved information while reducing hallucinations by fine-tuining with a custom LLM and an agent-driven approach for response evaluation and query engineering.\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mGiven the following conversation and a follow up question, rephrase the follow up question to be a standalone question, in its original language.\n",
      "\n",
      "Chat History:\n",
      "\n",
      "Human: tell me about lq rag\n",
      "Assistant: Context:\n",
      "1) Adaptation, and improved adherence to instructional instructions.\n",
      "2) A thorough review of relevant literature was conducted.\n",
      "3) The proposed RAG system was organized into two primary parts: fine-tuning (FT), which involves both embedding LLM and generating AA agents, and prompt engineering (PE), which is an evaluation agent and feedback mechanism.\n",
      "4) Experimental results demonstrate substantial improvements in FT Layer with advanced integration and fine-tuning techniques over baseline models, indicating the effectiveness of RAG system in retrieving contextually relevant information, resulting in more coherent answers.\n",
      "Human: what was my previous question?\n",
      "Assistant: Yes, I can provide more details on the fine-tuining process and how it contributes to the proposed RAG system's adoption in retrieving contextually relevant information.\n",
      "\n",
      "The FT layer of the proposed legal query RAG (LQ-RAG) is composed of several modules that are designed to fine-tune the embedded language model (LLM) and generate more accurate and diverse responses. The LLM module, which is responsible for generating textual embeddings, is first trained with a set of predefined input-output pairs in order to learn the underlying semantics of the given domain. This process involves a combination of image captioning and language modeling tasks to improve the LLM's ability to generate natural language responses.\n",
      "\n",
      "Once trained, the fine-tuining process is performed on each retrieved piece of information (e.g., a document, legal text, or court case) by the RAG layer. The RAG module learns to generate responses that are contextually relevant to the given information based on the output embeddings generated from the FT layer. The fine-tuining process involves several tasks, such as predicting the next word in a sentence or selecting the most likely response based on the contextual relevance of its preceding elements.\n",
      "\n",
      "The RAG module also incorporates an evaluation agent that analyzes and refines the generated responses through feedback. This feedback is provided by human annotators who provide suggestions for improving the RAG's performance, such as modifying the fine-tuining process or adding new modules to enhance the overall system's efficiency.\n",
      "\n",
      "In summary, LQ-RAG's proposed fine-tuining process enables it to generate contextually relevant and accurate responses from retrieved information, thereby improving its adoption in retrieving contextually relevant information.\n",
      "Human: what question i have asked you >\n",
      "Assistant: The provided context only addresses the Legal Query RAG system as a whole, not the specific performance evaluation of different user queries. In response generation, the fine-tuining process works by combining specialized LLMs with an agent-driven approach for response evaluation and query engineering. However, to address the limitations mentioned in the context, the LQ-RAG framework integrates a recursive feedback mechanism to overcome the critical shortcoming of standard RAG implementations in legal applications. The proposed system aims to generate more accurate and diverse responses from retrieved information while reducing hallucinations by fine-tuining with a custom LLM and an agent-driven approach for response evaluation and query engineering.\n",
      "Follow Up Input: escape\n",
      "Standalone question:\u001b[0m\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)\n",
      "Cell \u001b[1;32mIn[26], line 14\u001b[0m\n",
      "\u001b[0;32m      9\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m \u001b[38;5;66;03m# NEW\u001b[39;00m\n",
      "\u001b[0;32m     11\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m: \u001b[38;5;66;03m# NEW: Error handling for the loop\u001b[39;00m\n",
      "\u001b[0;32m     12\u001b[0m     \u001b[38;5;66;03m# CHANGED: Invoke the new conversational_rag_chain\u001b[39;00m\n",
      "\u001b[0;32m     13\u001b[0m     \u001b[38;5;66;03m# It now expects 'question' as input, and manages chat_history internally\u001b[39;00m\n",
      "\u001b[1;32m---> 14\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[43mconversational_rag_chain\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mquestion\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43muser_input\u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;32m     16\u001b[0m     ai_response_content \u001b[38;5;241m=\u001b[39m response[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124manswer\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;66;03m# CHANGED: Get answer from 'answer' key\u001b[39;00m\n",
      "\u001b[0;32m     17\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mAI Assistant: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mai_response_content\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "\n",
      "File \u001b[1;32mc:\\Users\\computer world\\Desktop\\Code\\Python\\langchain\\lib\\site-packages\\langchain\\chains\\base.py:167\u001b[0m, in \u001b[0;36mChain.invoke\u001b[1;34m(self, input, config, **kwargs)\u001b[0m\n",
      "\u001b[0;32m    165\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[0;32m    166\u001b[0m     run_manager\u001b[38;5;241m.\u001b[39mon_chain_error(e)\n",
      "\u001b[1;32m--> 167\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\n",
      "\u001b[0;32m    168\u001b[0m run_manager\u001b[38;5;241m.\u001b[39mon_chain_end(outputs)\n",
      "\u001b[0;32m    170\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m include_run_info:\n",
      "\n",
      "File \u001b[1;32mc:\\Users\\computer world\\Desktop\\Code\\Python\\langchain\\lib\\site-packages\\langchain\\chains\\base.py:157\u001b[0m, in \u001b[0;36mChain.invoke\u001b[1;34m(self, input, config, **kwargs)\u001b[0m\n",
      "\u001b[0;32m    154\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "\u001b[0;32m    155\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_validate_inputs(inputs)\n",
      "\u001b[0;32m    156\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m (\n",
      "\u001b[1;32m--> 157\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrun_manager\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;32m    158\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m new_arg_supported\n",
      "\u001b[0;32m    159\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call(inputs)\n",
      "\u001b[0;32m    160\u001b[0m     )\n",
      "\u001b[0;32m    162\u001b[0m     final_outputs: \u001b[38;5;28mdict\u001b[39m[\u001b[38;5;28mstr\u001b[39m, Any] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprep_outputs(\n",
      "\u001b[0;32m    163\u001b[0m         inputs, outputs, return_only_outputs\n",
      "\u001b[0;32m    164\u001b[0m     )\n",
      "\u001b[0;32m    165\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\n",
      "File \u001b[1;32mc:\\Users\\computer world\\Desktop\\Code\\Python\\langchain\\lib\\site-packages\\langchain\\chains\\conversational_retrieval\\base.py:150\u001b[0m, in \u001b[0;36mBaseConversationalRetrievalChain._call\u001b[1;34m(self, inputs, run_manager)\u001b[0m\n",
      "\u001b[0;32m    148\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m chat_history_str:\n",
      "\u001b[0;32m    149\u001b[0m     callbacks \u001b[38;5;241m=\u001b[39m _run_manager\u001b[38;5;241m.\u001b[39mget_child()\n",
      "\u001b[1;32m--> 150\u001b[0m     new_question \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mquestion_generator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\n",
      "\u001b[0;32m    151\u001b[0m \u001b[43m        \u001b[49m\u001b[43mquestion\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mquestion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mchat_history\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mchat_history_str\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallbacks\u001b[49m\n",
      "\u001b[0;32m    152\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;32m    153\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[0;32m    154\u001b[0m     new_question \u001b[38;5;241m=\u001b[39m question\n",
      "\n",
      "File \u001b[1;32mc:\\Users\\computer world\\Desktop\\Code\\Python\\langchain\\lib\\site-packages\\langchain_core\\_api\\deprecation.py:189\u001b[0m, in \u001b[0;36mdeprecated.<locals>.deprecate.<locals>.warning_emitting_wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[0;32m    187\u001b[0m     warned \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[0;32m    188\u001b[0m     emit_warning()\n",
      "\u001b[1;32m--> 189\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m wrapped(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "\n",
      "File \u001b[1;32mc:\\Users\\computer world\\Desktop\\Code\\Python\\langchain\\lib\\site-packages\\langchain\\chains\\base.py:608\u001b[0m, in \u001b[0;36mChain.run\u001b[1;34m(self, callbacks, tags, metadata, *args, **kwargs)\u001b[0m\n",
      "\u001b[0;32m    603\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m(args[\u001b[38;5;241m0\u001b[39m], callbacks\u001b[38;5;241m=\u001b[39mcallbacks, tags\u001b[38;5;241m=\u001b[39mtags, metadata\u001b[38;5;241m=\u001b[39mmetadata)[\n",
      "\u001b[0;32m    604\u001b[0m         _output_key\n",
      "\u001b[0;32m    605\u001b[0m     ]\n",
      "\u001b[0;32m    607\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m kwargs \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m args:\n",
      "\u001b[1;32m--> 608\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtags\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtags\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmetadata\u001b[49m\u001b[43m)\u001b[49m[\n",
      "\u001b[0;32m    609\u001b[0m         _output_key\n",
      "\u001b[0;32m    610\u001b[0m     ]\n",
      "\u001b[0;32m    612\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m kwargs \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m args:\n",
      "\u001b[0;32m    613\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n",
      "\u001b[0;32m    614\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`run` supported with either positional arguments or keyword arguments,\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "\u001b[0;32m    615\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m but none were provided.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "\u001b[0;32m    616\u001b[0m     )\n",
      "\n",
      "File \u001b[1;32mc:\\Users\\computer world\\Desktop\\Code\\Python\\langchain\\lib\\site-packages\\langchain_core\\_api\\deprecation.py:189\u001b[0m, in \u001b[0;36mdeprecated.<locals>.deprecate.<locals>.warning_emitting_wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[0;32m    187\u001b[0m     warned \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[0;32m    188\u001b[0m     emit_warning()\n",
      "\u001b[1;32m--> 189\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m wrapped(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "\n",
      "File \u001b[1;32mc:\\Users\\computer world\\Desktop\\Code\\Python\\langchain\\lib\\site-packages\\langchain\\chains\\base.py:386\u001b[0m, in \u001b[0;36mChain.__call__\u001b[1;34m(self, inputs, return_only_outputs, callbacks, tags, metadata, run_name, include_run_info)\u001b[0m\n",
      "\u001b[0;32m    354\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Execute the chain.\u001b[39;00m\n",
      "\u001b[0;32m    355\u001b[0m \n",
      "\u001b[0;32m    356\u001b[0m \u001b[38;5;124;03mArgs:\u001b[39;00m\n",
      "\u001b[1;32m   (...)\u001b[0m\n",
      "\u001b[0;32m    377\u001b[0m \u001b[38;5;124;03m        `Chain.output_keys`.\u001b[39;00m\n",
      "\u001b[0;32m    378\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n",
      "\u001b[0;32m    379\u001b[0m config \u001b[38;5;241m=\u001b[39m {\n",
      "\u001b[0;32m    380\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcallbacks\u001b[39m\u001b[38;5;124m\"\u001b[39m: callbacks,\n",
      "\u001b[0;32m    381\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtags\u001b[39m\u001b[38;5;124m\"\u001b[39m: tags,\n",
      "\u001b[0;32m    382\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmetadata\u001b[39m\u001b[38;5;124m\"\u001b[39m: metadata,\n",
      "\u001b[0;32m    383\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrun_name\u001b[39m\u001b[38;5;124m\"\u001b[39m: run_name,\n",
      "\u001b[0;32m    384\u001b[0m }\n",
      "\u001b[1;32m--> 386\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\n",
      "\u001b[0;32m    387\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[0;32m    388\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcast\u001b[49m\u001b[43m(\u001b[49m\u001b[43mRunnableConfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\u001b[43mk\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mv\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mk\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mv\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitems\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mv\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[0;32m    389\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_only_outputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_only_outputs\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[0;32m    390\u001b[0m \u001b[43m    \u001b[49m\u001b[43minclude_run_info\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minclude_run_info\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[0;32m    391\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\n",
      "File \u001b[1;32mc:\\Users\\computer world\\Desktop\\Code\\Python\\langchain\\lib\\site-packages\\langchain\\chains\\base.py:167\u001b[0m, in \u001b[0;36mChain.invoke\u001b[1;34m(self, input, config, **kwargs)\u001b[0m\n",
      "\u001b[0;32m    165\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[0;32m    166\u001b[0m     run_manager\u001b[38;5;241m.\u001b[39mon_chain_error(e)\n",
      "\u001b[1;32m--> 167\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\n",
      "\u001b[0;32m    168\u001b[0m run_manager\u001b[38;5;241m.\u001b[39mon_chain_end(outputs)\n",
      "\u001b[0;32m    170\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m include_run_info:\n",
      "\n",
      "File \u001b[1;32mc:\\Users\\computer world\\Desktop\\Code\\Python\\langchain\\lib\\site-packages\\langchain\\chains\\base.py:157\u001b[0m, in \u001b[0;36mChain.invoke\u001b[1;34m(self, input, config, **kwargs)\u001b[0m\n",
      "\u001b[0;32m    154\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "\u001b[0;32m    155\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_validate_inputs(inputs)\n",
      "\u001b[0;32m    156\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m (\n",
      "\u001b[1;32m--> 157\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrun_manager\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;32m    158\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m new_arg_supported\n",
      "\u001b[0;32m    159\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call(inputs)\n",
      "\u001b[0;32m    160\u001b[0m     )\n",
      "\u001b[0;32m    162\u001b[0m     final_outputs: \u001b[38;5;28mdict\u001b[39m[\u001b[38;5;28mstr\u001b[39m, Any] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprep_outputs(\n",
      "\u001b[0;32m    163\u001b[0m         inputs, outputs, return_only_outputs\n",
      "\u001b[0;32m    164\u001b[0m     )\n",
      "\u001b[0;32m    165\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\n",
      "File \u001b[1;32mc:\\Users\\computer world\\Desktop\\Code\\Python\\langchain\\lib\\site-packages\\langchain\\chains\\llm.py:127\u001b[0m, in \u001b[0;36mLLMChain._call\u001b[1;34m(self, inputs, run_manager)\u001b[0m\n",
      "\u001b[0;32m    122\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_call\u001b[39m(\n",
      "\u001b[0;32m    123\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n",
      "\u001b[0;32m    124\u001b[0m     inputs: \u001b[38;5;28mdict\u001b[39m[\u001b[38;5;28mstr\u001b[39m, Any],\n",
      "\u001b[0;32m    125\u001b[0m     run_manager: Optional[CallbackManagerForChainRun] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n",
      "\u001b[0;32m    126\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mdict\u001b[39m[\u001b[38;5;28mstr\u001b[39m, \u001b[38;5;28mstr\u001b[39m]:\n",
      "\u001b[1;32m--> 127\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrun_manager\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;32m    128\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcreate_outputs(response)[\u001b[38;5;241m0\u001b[39m]\n",
      "\n",
      "File \u001b[1;32mc:\\Users\\computer world\\Desktop\\Code\\Python\\langchain\\lib\\site-packages\\langchain\\chains\\llm.py:139\u001b[0m, in \u001b[0;36mLLMChain.generate\u001b[1;34m(self, input_list, run_manager)\u001b[0m\n",
      "\u001b[0;32m    137\u001b[0m callbacks \u001b[38;5;241m=\u001b[39m run_manager\u001b[38;5;241m.\u001b[39mget_child() \u001b[38;5;28;01mif\u001b[39;00m run_manager \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[0;32m    138\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mllm, BaseLanguageModel):\n",
      "\u001b[1;32m--> 139\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mllm\u001b[38;5;241m.\u001b[39mgenerate_prompt(\n",
      "\u001b[0;32m    140\u001b[0m         prompts,\n",
      "\u001b[0;32m    141\u001b[0m         stop,\n",
      "\u001b[0;32m    142\u001b[0m         callbacks\u001b[38;5;241m=\u001b[39mcallbacks,\n",
      "\u001b[0;32m    143\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mllm_kwargs,\n",
      "\u001b[0;32m    144\u001b[0m     )\n",
      "\u001b[0;32m    145\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[0;32m    146\u001b[0m     results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mllm\u001b[38;5;241m.\u001b[39mbind(stop\u001b[38;5;241m=\u001b[39mstop, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mllm_kwargs)\u001b[38;5;241m.\u001b[39mbatch(\n",
      "\u001b[0;32m    147\u001b[0m         cast(\u001b[38;5;28mlist\u001b[39m, prompts), {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcallbacks\u001b[39m\u001b[38;5;124m\"\u001b[39m: callbacks}\n",
      "\u001b[0;32m    148\u001b[0m     )\n",
      "\n",
      "File \u001b[1;32mc:\\Users\\computer world\\Desktop\\Code\\Python\\langchain\\lib\\site-packages\\langchain_core\\language_models\\llms.py:766\u001b[0m, in \u001b[0;36mBaseLLM.generate_prompt\u001b[1;34m(self, prompts, stop, callbacks, **kwargs)\u001b[0m\n",
      "\u001b[0;32m    757\u001b[0m \u001b[38;5;129m@override\u001b[39m\n",
      "\u001b[0;32m    758\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mgenerate_prompt\u001b[39m(\n",
      "\u001b[0;32m    759\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n",
      "\u001b[1;32m   (...)\u001b[0m\n",
      "\u001b[0;32m    763\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n",
      "\u001b[0;32m    764\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m LLMResult:\n",
      "\u001b[0;32m    765\u001b[0m     prompt_strings \u001b[38;5;241m=\u001b[39m [p\u001b[38;5;241m.\u001b[39mto_string() \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m prompts]\n",
      "\u001b[1;32m--> 766\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgenerate(prompt_strings, stop\u001b[38;5;241m=\u001b[39mstop, callbacks\u001b[38;5;241m=\u001b[39mcallbacks, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "\n",
      "File \u001b[1;32mc:\\Users\\computer world\\Desktop\\Code\\Python\\langchain\\lib\\site-packages\\langchain_core\\language_models\\llms.py:973\u001b[0m, in \u001b[0;36mBaseLLM.generate\u001b[1;34m(self, prompts, stop, callbacks, tags, metadata, run_name, run_id, **kwargs)\u001b[0m\n",
      "\u001b[0;32m    958\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcache \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m get_llm_cache() \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcache \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m:\n",
      "\u001b[0;32m    959\u001b[0m     run_managers \u001b[38;5;241m=\u001b[39m [\n",
      "\u001b[0;32m    960\u001b[0m         callback_manager\u001b[38;5;241m.\u001b[39mon_llm_start(\n",
      "\u001b[0;32m    961\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_serialized,\n",
      "\u001b[1;32m   (...)\u001b[0m\n",
      "\u001b[0;32m    971\u001b[0m         )\n",
      "\u001b[0;32m    972\u001b[0m     ]\n",
      "\u001b[1;32m--> 973\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_generate_helper(\n",
      "\u001b[0;32m    974\u001b[0m         prompts,\n",
      "\u001b[0;32m    975\u001b[0m         stop,\n",
      "\u001b[0;32m    976\u001b[0m         run_managers,\n",
      "\u001b[0;32m    977\u001b[0m         new_arg_supported\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mbool\u001b[39m(new_arg_supported),\n",
      "\u001b[0;32m    978\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n",
      "\u001b[0;32m    979\u001b[0m     )\n",
      "\u001b[0;32m    980\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(missing_prompts) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "\u001b[0;32m    981\u001b[0m     run_managers \u001b[38;5;241m=\u001b[39m [\n",
      "\u001b[0;32m    982\u001b[0m         callback_managers[idx]\u001b[38;5;241m.\u001b[39mon_llm_start(\n",
      "\u001b[0;32m    983\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_serialized,\n",
      "\u001b[1;32m   (...)\u001b[0m\n",
      "\u001b[0;32m    990\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m missing_prompt_idxs\n",
      "\u001b[0;32m    991\u001b[0m     ]\n",
      "\n",
      "File \u001b[1;32mc:\\Users\\computer world\\Desktop\\Code\\Python\\langchain\\lib\\site-packages\\langchain_core\\language_models\\llms.py:792\u001b[0m, in \u001b[0;36mBaseLLM._generate_helper\u001b[1;34m(self, prompts, stop, run_managers, new_arg_supported, **kwargs)\u001b[0m\n",
      "\u001b[0;32m    781\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_generate_helper\u001b[39m(\n",
      "\u001b[0;32m    782\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n",
      "\u001b[0;32m    783\u001b[0m     prompts: \u001b[38;5;28mlist\u001b[39m[\u001b[38;5;28mstr\u001b[39m],\n",
      "\u001b[1;32m   (...)\u001b[0m\n",
      "\u001b[0;32m    788\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n",
      "\u001b[0;32m    789\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m LLMResult:\n",
      "\u001b[0;32m    790\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "\u001b[0;32m    791\u001b[0m         output \u001b[38;5;241m=\u001b[39m (\n",
      "\u001b[1;32m--> 792\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_generate(\n",
      "\u001b[0;32m    793\u001b[0m                 prompts,\n",
      "\u001b[0;32m    794\u001b[0m                 stop\u001b[38;5;241m=\u001b[39mstop,\n",
      "\u001b[0;32m    795\u001b[0m                 \u001b[38;5;66;03m# TODO: support multiple run managers\u001b[39;00m\n",
      "\u001b[0;32m    796\u001b[0m                 run_manager\u001b[38;5;241m=\u001b[39mrun_managers[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m run_managers \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n",
      "\u001b[0;32m    797\u001b[0m                 \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n",
      "\u001b[0;32m    798\u001b[0m             )\n",
      "\u001b[0;32m    799\u001b[0m             \u001b[38;5;28;01mif\u001b[39;00m new_arg_supported\n",
      "\u001b[0;32m    800\u001b[0m             \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_generate(prompts, stop\u001b[38;5;241m=\u001b[39mstop)\n",
      "\u001b[0;32m    801\u001b[0m         )\n",
      "\u001b[0;32m    802\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[0;32m    803\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m run_manager \u001b[38;5;129;01min\u001b[39;00m run_managers:\n",
      "\n",
      "File \u001b[1;32mc:\\Users\\computer world\\Desktop\\Code\\Python\\langchain\\lib\\site-packages\\langchain_ollama\\llms.py:359\u001b[0m, in \u001b[0;36mOllamaLLM._generate\u001b[1;34m(self, prompts, stop, run_manager, **kwargs)\u001b[0m\n",
      "\u001b[0;32m    357\u001b[0m generations \u001b[38;5;241m=\u001b[39m []\n",
      "\u001b[0;32m    358\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m prompt \u001b[38;5;129;01min\u001b[39;00m prompts:\n",
      "\u001b[1;32m--> 359\u001b[0m     final_chunk \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_stream_with_aggregation(\n",
      "\u001b[0;32m    360\u001b[0m         prompt,\n",
      "\u001b[0;32m    361\u001b[0m         stop\u001b[38;5;241m=\u001b[39mstop,\n",
      "\u001b[0;32m    362\u001b[0m         run_manager\u001b[38;5;241m=\u001b[39mrun_manager,\n",
      "\u001b[0;32m    363\u001b[0m         verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mverbose,\n",
      "\u001b[0;32m    364\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n",
      "\u001b[0;32m    365\u001b[0m     )\n",
      "\u001b[0;32m    366\u001b[0m     generations\u001b[38;5;241m.\u001b[39mappend([final_chunk])\n",
      "\u001b[0;32m    367\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m LLMResult(generations\u001b[38;5;241m=\u001b[39mgenerations)\n",
      "\n",
      "File \u001b[1;32mc:\\Users\\computer world\\Desktop\\Code\\Python\\langchain\\lib\\site-packages\\langchain_ollama\\llms.py:318\u001b[0m, in \u001b[0;36mOllamaLLM._stream_with_aggregation\u001b[1;34m(self, prompt, stop, run_manager, verbose, **kwargs)\u001b[0m\n",
      "\u001b[0;32m    316\u001b[0m final_chunk \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[0;32m    317\u001b[0m thinking_content \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "\u001b[1;32m--> 318\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m stream_resp \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_create_generate_stream(prompt, stop, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n",
      "\u001b[0;32m    319\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(stream_resp, \u001b[38;5;28mstr\u001b[39m):\n",
      "\u001b[0;32m    320\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m stream_resp\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mthinking\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "\n",
      "File \u001b[1;32mc:\\Users\\computer world\\Desktop\\Code\\Python\\langchain\\lib\\site-packages\\langchain_ollama\\llms.py:262\u001b[0m, in \u001b[0;36mOllamaLLM._create_generate_stream\u001b[1;34m(self, prompt, stop, **kwargs)\u001b[0m\n",
      "\u001b[0;32m    255\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_create_generate_stream\u001b[39m(\n",
      "\u001b[0;32m    256\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n",
      "\u001b[0;32m    257\u001b[0m     prompt: \u001b[38;5;28mstr\u001b[39m,\n",
      "\u001b[0;32m    258\u001b[0m     stop: Optional[\u001b[38;5;28mlist\u001b[39m[\u001b[38;5;28mstr\u001b[39m]] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n",
      "\u001b[0;32m    259\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n",
      "\u001b[0;32m    260\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Iterator[Union[Mapping[\u001b[38;5;28mstr\u001b[39m, Any], \u001b[38;5;28mstr\u001b[39m]]:\n",
      "\u001b[0;32m    261\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_client:\n",
      "\u001b[1;32m--> 262\u001b[0m         \u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_client\u001b[38;5;241m.\u001b[39mgenerate(\n",
      "\u001b[0;32m    263\u001b[0m             \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_generate_params(prompt, stop\u001b[38;5;241m=\u001b[39mstop, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "\u001b[0;32m    264\u001b[0m         )\n",
      "\n",
      "File \u001b[1;32mc:\\Users\\computer world\\Desktop\\Code\\Python\\langchain\\lib\\site-packages\\ollama\\_client.py:165\u001b[0m, in \u001b[0;36mClient._request.<locals>.inner\u001b[1;34m()\u001b[0m\n",
      "\u001b[0;32m    164\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21minner\u001b[39m():\n",
      "\u001b[1;32m--> 165\u001b[0m   \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_client\u001b[38;5;241m.\u001b[39mstream(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs) \u001b[38;5;28;01mas\u001b[39;00m r:\n",
      "\u001b[0;32m    166\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "\u001b[0;32m    167\u001b[0m       r\u001b[38;5;241m.\u001b[39mraise_for_status()\n",
      "\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\contextlib.py:135\u001b[0m, in \u001b[0;36m_GeneratorContextManager.__enter__\u001b[1;34m(self)\u001b[0m\n",
      "\u001b[0;32m    133\u001b[0m \u001b[38;5;28;01mdel\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mkwds, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfunc\n",
      "\u001b[0;32m    134\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "\u001b[1;32m--> 135\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgen\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;32m    136\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m:\n",
      "\u001b[0;32m    137\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgenerator didn\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt yield\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\n",
      "File \u001b[1;32mc:\\Users\\computer world\\Desktop\\Code\\Python\\langchain\\lib\\site-packages\\httpx\\_client.py:868\u001b[0m, in \u001b[0;36mClient.stream\u001b[1;34m(self, method, url, content, data, files, json, params, headers, cookies, auth, follow_redirects, timeout, extensions)\u001b[0m\n",
      "\u001b[0;32m    845\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n",
      "\u001b[0;32m    846\u001b[0m \u001b[38;5;124;03mAlternative to `httpx.request()` that streams the response body\u001b[39;00m\n",
      "\u001b[0;32m    847\u001b[0m \u001b[38;5;124;03minstead of loading it into memory at once.\u001b[39;00m\n",
      "\u001b[1;32m   (...)\u001b[0m\n",
      "\u001b[0;32m    853\u001b[0m \u001b[38;5;124;03m[0]: /quickstart#streaming-responses\u001b[39;00m\n",
      "\u001b[0;32m    854\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n",
      "\u001b[0;32m    855\u001b[0m request \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbuild_request(\n",
      "\u001b[0;32m    856\u001b[0m     method\u001b[38;5;241m=\u001b[39mmethod,\n",
      "\u001b[0;32m    857\u001b[0m     url\u001b[38;5;241m=\u001b[39murl,\n",
      "\u001b[1;32m   (...)\u001b[0m\n",
      "\u001b[0;32m    866\u001b[0m     extensions\u001b[38;5;241m=\u001b[39mextensions,\n",
      "\u001b[0;32m    867\u001b[0m )\n",
      "\u001b[1;32m--> 868\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\n",
      "\u001b[0;32m    869\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrequest\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[0;32m    870\u001b[0m \u001b[43m    \u001b[49m\u001b[43mauth\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mauth\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[0;32m    871\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfollow_redirects\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfollow_redirects\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[0;32m    872\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n",
      "\u001b[0;32m    873\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;32m    874\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "\u001b[0;32m    875\u001b[0m     \u001b[38;5;28;01myield\u001b[39;00m response\n",
      "\n",
      "File \u001b[1;32mc:\\Users\\computer world\\Desktop\\Code\\Python\\langchain\\lib\\site-packages\\httpx\\_client.py:914\u001b[0m, in \u001b[0;36mClient.send\u001b[1;34m(self, request, stream, auth, follow_redirects)\u001b[0m\n",
      "\u001b[0;32m    910\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_set_timeout(request)\n",
      "\u001b[0;32m    912\u001b[0m auth \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_build_request_auth(request, auth)\n",
      "\u001b[1;32m--> 914\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_send_handling_auth\u001b[49m\u001b[43m(\u001b[49m\n",
      "\u001b[0;32m    915\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[0;32m    916\u001b[0m \u001b[43m    \u001b[49m\u001b[43mauth\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mauth\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[0;32m    917\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfollow_redirects\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfollow_redirects\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[0;32m    918\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhistory\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[0;32m    919\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;32m    920\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "\u001b[0;32m    921\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m stream:\n",
      "\n",
      "File \u001b[1;32mc:\\Users\\computer world\\Desktop\\Code\\Python\\langchain\\lib\\site-packages\\httpx\\_client.py:942\u001b[0m, in \u001b[0;36mClient._send_handling_auth\u001b[1;34m(self, request, auth, follow_redirects, history)\u001b[0m\n",
      "\u001b[0;32m    939\u001b[0m request \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mnext\u001b[39m(auth_flow)\n",
      "\u001b[0;32m    941\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n",
      "\u001b[1;32m--> 942\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_send_handling_redirects\u001b[49m\u001b[43m(\u001b[49m\n",
      "\u001b[0;32m    943\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[0;32m    944\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfollow_redirects\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfollow_redirects\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[0;32m    945\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhistory\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhistory\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[0;32m    946\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;32m    947\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "\u001b[0;32m    948\u001b[0m         \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "\n",
      "File \u001b[1;32mc:\\Users\\computer world\\Desktop\\Code\\Python\\langchain\\lib\\site-packages\\httpx\\_client.py:979\u001b[0m, in \u001b[0;36mClient._send_handling_redirects\u001b[1;34m(self, request, follow_redirects, history)\u001b[0m\n",
      "\u001b[0;32m    976\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m hook \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_event_hooks[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrequest\u001b[39m\u001b[38;5;124m\"\u001b[39m]:\n",
      "\u001b[0;32m    977\u001b[0m     hook(request)\n",
      "\u001b[1;32m--> 979\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_send_single_request\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;32m    980\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "\u001b[0;32m    981\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m hook \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_event_hooks[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mresponse\u001b[39m\u001b[38;5;124m\"\u001b[39m]:\n",
      "\n",
      "File \u001b[1;32mc:\\Users\\computer world\\Desktop\\Code\\Python\\langchain\\lib\\site-packages\\httpx\\_client.py:1014\u001b[0m, in \u001b[0;36mClient._send_single_request\u001b[1;34m(self, request)\u001b[0m\n",
      "\u001b[0;32m   1009\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n",
      "\u001b[0;32m   1010\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAttempted to send an async request with a sync Client instance.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "\u001b[0;32m   1011\u001b[0m     )\n",
      "\u001b[0;32m   1013\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m request_context(request\u001b[38;5;241m=\u001b[39mrequest):\n",
      "\u001b[1;32m-> 1014\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[43mtransport\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhandle_request\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;32m   1016\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(response\u001b[38;5;241m.\u001b[39mstream, SyncByteStream)\n",
      "\u001b[0;32m   1018\u001b[0m response\u001b[38;5;241m.\u001b[39mrequest \u001b[38;5;241m=\u001b[39m request\n",
      "\n",
      "File \u001b[1;32mc:\\Users\\computer world\\Desktop\\Code\\Python\\langchain\\lib\\site-packages\\httpx\\_transports\\default.py:250\u001b[0m, in \u001b[0;36mHTTPTransport.handle_request\u001b[1;34m(self, request)\u001b[0m\n",
      "\u001b[0;32m    237\u001b[0m req \u001b[38;5;241m=\u001b[39m httpcore\u001b[38;5;241m.\u001b[39mRequest(\n",
      "\u001b[0;32m    238\u001b[0m     method\u001b[38;5;241m=\u001b[39mrequest\u001b[38;5;241m.\u001b[39mmethod,\n",
      "\u001b[0;32m    239\u001b[0m     url\u001b[38;5;241m=\u001b[39mhttpcore\u001b[38;5;241m.\u001b[39mURL(\n",
      "\u001b[1;32m   (...)\u001b[0m\n",
      "\u001b[0;32m    247\u001b[0m     extensions\u001b[38;5;241m=\u001b[39mrequest\u001b[38;5;241m.\u001b[39mextensions,\n",
      "\u001b[0;32m    248\u001b[0m )\n",
      "\u001b[0;32m    249\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m map_httpcore_exceptions():\n",
      "\u001b[1;32m--> 250\u001b[0m     resp \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_pool\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhandle_request\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreq\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;32m    252\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(resp\u001b[38;5;241m.\u001b[39mstream, typing\u001b[38;5;241m.\u001b[39mIterable)\n",
      "\u001b[0;32m    254\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m Response(\n",
      "\u001b[0;32m    255\u001b[0m     status_code\u001b[38;5;241m=\u001b[39mresp\u001b[38;5;241m.\u001b[39mstatus,\n",
      "\u001b[0;32m    256\u001b[0m     headers\u001b[38;5;241m=\u001b[39mresp\u001b[38;5;241m.\u001b[39mheaders,\n",
      "\u001b[0;32m    257\u001b[0m     stream\u001b[38;5;241m=\u001b[39mResponseStream(resp\u001b[38;5;241m.\u001b[39mstream),\n",
      "\u001b[0;32m    258\u001b[0m     extensions\u001b[38;5;241m=\u001b[39mresp\u001b[38;5;241m.\u001b[39mextensions,\n",
      "\u001b[0;32m    259\u001b[0m )\n",
      "\n",
      "File \u001b[1;32mc:\\Users\\computer world\\Desktop\\Code\\Python\\langchain\\lib\\site-packages\\httpcore\\_sync\\connection_pool.py:256\u001b[0m, in \u001b[0;36mConnectionPool.handle_request\u001b[1;34m(self, request)\u001b[0m\n",
      "\u001b[0;32m    253\u001b[0m         closing \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_assign_requests_to_connections()\n",
      "\u001b[0;32m    255\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_close_connections(closing)\n",
      "\u001b[1;32m--> 256\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m exc \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[0;32m    258\u001b[0m \u001b[38;5;66;03m# Return the response. Note that in this case we still have to manage\u001b[39;00m\n",
      "\u001b[0;32m    259\u001b[0m \u001b[38;5;66;03m# the point at which the response is closed.\u001b[39;00m\n",
      "\u001b[0;32m    260\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(response\u001b[38;5;241m.\u001b[39mstream, typing\u001b[38;5;241m.\u001b[39mIterable)\n",
      "\n",
      "File \u001b[1;32mc:\\Users\\computer world\\Desktop\\Code\\Python\\langchain\\lib\\site-packages\\httpcore\\_sync\\connection_pool.py:236\u001b[0m, in \u001b[0;36mConnectionPool.handle_request\u001b[1;34m(self, request)\u001b[0m\n",
      "\u001b[0;32m    232\u001b[0m connection \u001b[38;5;241m=\u001b[39m pool_request\u001b[38;5;241m.\u001b[39mwait_for_connection(timeout\u001b[38;5;241m=\u001b[39mtimeout)\n",
      "\u001b[0;32m    234\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "\u001b[0;32m    235\u001b[0m     \u001b[38;5;66;03m# Send the request on the assigned connection.\u001b[39;00m\n",
      "\u001b[1;32m--> 236\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[43mconnection\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhandle_request\u001b[49m\u001b[43m(\u001b[49m\n",
      "\u001b[0;32m    237\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpool_request\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequest\u001b[49m\n",
      "\u001b[0;32m    238\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;32m    239\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m ConnectionNotAvailable:\n",
      "\u001b[0;32m    240\u001b[0m     \u001b[38;5;66;03m# In some cases a connection may initially be available to\u001b[39;00m\n",
      "\u001b[0;32m    241\u001b[0m     \u001b[38;5;66;03m# handle a request, but then become unavailable.\u001b[39;00m\n",
      "\u001b[0;32m    242\u001b[0m     \u001b[38;5;66;03m#\u001b[39;00m\n",
      "\u001b[0;32m    243\u001b[0m     \u001b[38;5;66;03m# In this case we clear the connection and try again.\u001b[39;00m\n",
      "\u001b[0;32m    244\u001b[0m     pool_request\u001b[38;5;241m.\u001b[39mclear_connection()\n",
      "\n",
      "File \u001b[1;32mc:\\Users\\computer world\\Desktop\\Code\\Python\\langchain\\lib\\site-packages\\httpcore\\_sync\\connection.py:103\u001b[0m, in \u001b[0;36mHTTPConnection.handle_request\u001b[1;34m(self, request)\u001b[0m\n",
      "\u001b[0;32m    100\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_connect_failed \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[0;32m    101\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m exc\n",
      "\u001b[1;32m--> 103\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_connection\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhandle_request\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m)\u001b[49m\n",
      "\n",
      "File \u001b[1;32mc:\\Users\\computer world\\Desktop\\Code\\Python\\langchain\\lib\\site-packages\\httpcore\\_sync\\http11.py:136\u001b[0m, in \u001b[0;36mHTTP11Connection.handle_request\u001b[1;34m(self, request)\u001b[0m\n",
      "\u001b[0;32m    134\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m Trace(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mresponse_closed\u001b[39m\u001b[38;5;124m\"\u001b[39m, logger, request) \u001b[38;5;28;01mas\u001b[39;00m trace:\n",
      "\u001b[0;32m    135\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_response_closed()\n",
      "\u001b[1;32m--> 136\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m exc\n",
      "\n",
      "File \u001b[1;32mc:\\Users\\computer world\\Desktop\\Code\\Python\\langchain\\lib\\site-packages\\httpcore\\_sync\\http11.py:106\u001b[0m, in \u001b[0;36mHTTP11Connection.handle_request\u001b[1;34m(self, request)\u001b[0m\n",
      "\u001b[0;32m     95\u001b[0m     \u001b[38;5;28;01mpass\u001b[39;00m\n",
      "\u001b[0;32m     97\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m Trace(\n",
      "\u001b[0;32m     98\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mreceive_response_headers\u001b[39m\u001b[38;5;124m\"\u001b[39m, logger, request, kwargs\n",
      "\u001b[0;32m     99\u001b[0m ) \u001b[38;5;28;01mas\u001b[39;00m trace:\n",
      "\u001b[0;32m    100\u001b[0m     (\n",
      "\u001b[0;32m    101\u001b[0m         http_version,\n",
      "\u001b[0;32m    102\u001b[0m         status,\n",
      "\u001b[0;32m    103\u001b[0m         reason_phrase,\n",
      "\u001b[0;32m    104\u001b[0m         headers,\n",
      "\u001b[0;32m    105\u001b[0m         trailing_data,\n",
      "\u001b[1;32m--> 106\u001b[0m     ) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_receive_response_headers(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "\u001b[0;32m    107\u001b[0m     trace\u001b[38;5;241m.\u001b[39mreturn_value \u001b[38;5;241m=\u001b[39m (\n",
      "\u001b[0;32m    108\u001b[0m         http_version,\n",
      "\u001b[0;32m    109\u001b[0m         status,\n",
      "\u001b[0;32m    110\u001b[0m         reason_phrase,\n",
      "\u001b[0;32m    111\u001b[0m         headers,\n",
      "\u001b[0;32m    112\u001b[0m     )\n",
      "\u001b[0;32m    114\u001b[0m network_stream \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_network_stream\n",
      "\n",
      "File \u001b[1;32mc:\\Users\\computer world\\Desktop\\Code\\Python\\langchain\\lib\\site-packages\\httpcore\\_sync\\http11.py:177\u001b[0m, in \u001b[0;36mHTTP11Connection._receive_response_headers\u001b[1;34m(self, request)\u001b[0m\n",
      "\u001b[0;32m    174\u001b[0m timeout \u001b[38;5;241m=\u001b[39m timeouts\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mread\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n",
      "\u001b[0;32m    176\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n",
      "\u001b[1;32m--> 177\u001b[0m     event \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_receive_event\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;32m    178\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(event, h11\u001b[38;5;241m.\u001b[39mResponse):\n",
      "\u001b[0;32m    179\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "\n",
      "File \u001b[1;32mc:\\Users\\computer world\\Desktop\\Code\\Python\\langchain\\lib\\site-packages\\httpcore\\_sync\\http11.py:217\u001b[0m, in \u001b[0;36mHTTP11Connection._receive_event\u001b[1;34m(self, timeout)\u001b[0m\n",
      "\u001b[0;32m    214\u001b[0m     event \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_h11_state\u001b[38;5;241m.\u001b[39mnext_event()\n",
      "\u001b[0;32m    216\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m event \u001b[38;5;129;01mis\u001b[39;00m h11\u001b[38;5;241m.\u001b[39mNEED_DATA:\n",
      "\u001b[1;32m--> 217\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_network_stream\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\n",
      "\u001b[0;32m    218\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mREAD_NUM_BYTES\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\n",
      "\u001b[0;32m    219\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;32m    221\u001b[0m     \u001b[38;5;66;03m# If we feed this case through h11 we'll raise an exception like:\u001b[39;00m\n",
      "\u001b[0;32m    222\u001b[0m     \u001b[38;5;66;03m#\u001b[39;00m\n",
      "\u001b[0;32m    223\u001b[0m     \u001b[38;5;66;03m#     httpcore.RemoteProtocolError: can't handle event type\u001b[39;00m\n",
      "\u001b[1;32m   (...)\u001b[0m\n",
      "\u001b[0;32m    227\u001b[0m     \u001b[38;5;66;03m# perspective. Instead we handle this case distinctly and treat\u001b[39;00m\n",
      "\u001b[0;32m    228\u001b[0m     \u001b[38;5;66;03m# it as a ConnectError.\u001b[39;00m\n",
      "\u001b[0;32m    229\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m data \u001b[38;5;241m==\u001b[39m \u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_h11_state\u001b[38;5;241m.\u001b[39mtheir_state \u001b[38;5;241m==\u001b[39m h11\u001b[38;5;241m.\u001b[39mSEND_RESPONSE:\n",
      "\n",
      "File \u001b[1;32mc:\\Users\\computer world\\Desktop\\Code\\Python\\langchain\\lib\\site-packages\\httpcore\\_backends\\sync.py:128\u001b[0m, in \u001b[0;36mSyncStream.read\u001b[1;34m(self, max_bytes, timeout)\u001b[0m\n",
      "\u001b[0;32m    126\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m map_exceptions(exc_map):\n",
      "\u001b[0;32m    127\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sock\u001b[38;5;241m.\u001b[39msettimeout(timeout)\n",
      "\u001b[1;32m--> 128\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sock\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrecv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmax_bytes\u001b[49m\u001b[43m)\u001b[49m\n",
      "\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "print(\"\\n--- AI Assistant (Conversational PDF Search): Type 'exit' or 'quit' to end the conversation. ---\") # CHANGED: Initial message\n",
    "print(\"Hello! How can I help you with your legal document today?\") # CHANGED: Initial message\n",
    "\n",
    "while True: # NEW: The main conversational loop\n",
    "    user_input = input(\"\\nYou: \") # NEW: Get user input\n",
    "\n",
    "    if user_input.lower() in [\"exit\", \"quit\"]: # NEW: Exit condition\n",
    "        print(\"AI Assistant: Goodbye!\") # NEW\n",
    "        break # NEW\n",
    "\n",
    "    try: # NEW: Error handling for the loop\n",
    "        # CHANGED: Invoke the new conversational_rag_chain\n",
    "        # It now expects 'question' as input, and manages chat_history internally\n",
    "        response = conversational_rag_chain.invoke({\"question\": user_input})\n",
    "\n",
    "        ai_response_content = response['answer'] # CHANGED: Get answer from 'answer' key\n",
    "        print(f\"\\nAI Assistant: {ai_response_content}\")\n",
    "\n",
    "    except Exception as e: # NEW: Error handling\n",
    "        print(f\"\\nAI Assistant: An error occurred: {e}\")\n",
    "        print(\"AI Assistant: Please try rephrasing your question or type 'exit' to end.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f6f6796",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Invoking the RAG Chain ---\n"
     ]
    }
   ],
   "source": [
    "# Invoking the RAG Chain with the Query \n",
    "print(\"\\n--- Invoking the RAG Chain ---\")\n",
    "query_text = \"tell us about the LQ RAG?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0204bc41",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Your Question:\n",
      "tell us about the LQ RAG?\n",
      "\n",
      "AI's Answer:\n",
      "Question: What are the different components of the proposed legal quotation generation system, including the fine-tuning layers and evaluation agents in the RAG layer? Answer:\n",
      "\n",
      "Section III of the paper provides a detailed explanation for the various components of the proposed legal quotation generation system, including the fine-tuining layers and evaluation agents. The section introduces the Fine-Tuining (FT) layer, which involves fine-tuining both the embedding LLM and the generative LLM in a systematic way to optimize their performance. This is followed by the RAG layer, which includes advanced RAG modules, an evaluation agent, a prompt engineering agent, and a feedback mechanism to ensure the quality and accuracy of generated responses. The bottom left quadrant of the FT Layer illustrates human inputs, which are fed into the system to evaluate its performance and improve its capabilities.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#.invoke() method will run the entire RAG pipeline\n",
    "response = retrieval_chain.invoke({\"input\": query_text})\n",
    "\n",
    "print(f\"\\nYour Question:\\n{query_text}\")\n",
    "print(f\"\\nAI's Answer:\\n{response['answer']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0194089",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Retrieved Documents ---\n",
      "adaptation, and improved adherence to instructions.\n",
      "4) Extensive evaluations were performed to assess the\n",
      "performance of the proposed RAG system. The results\n",
      "demonstrate that LQ-RAG consistently outpe ...\n",
      "---\n",
      "LQ-RAG is depicted in Algorithm 3.\n",
      "Algorithm 1Embedding LLM Fine-Tuning Process\n",
      "Constants: Loss function MNRL, Evaluator Eval,\n",
      "Learning Rate η\n",
      "Input: Csub-legal\n",
      "Output: Trained LLM network parameters  ...\n",
      "---\n",
      "of 80%, showing a 23% improvement over Naive RAG and\n",
      "a 14% improvement over RAG with FTM. This substantial\n",
      "improvement is attributed to the advanced integration and\n",
      "fine-tuning techniques in LQ-RAG, w ...\n",
      "---\n",
      "proposed LQ-RAG system. The proposed system is organized\n",
      "into two primary parts: Fine-Tuning (FT) Layer and RAG\n",
      "Layer. The FT Layer involves fine-tuning both the embedding\n",
      "LLM and the generative LLM.  ...\n",
      "---\n"
     ]
    }
   ],
   "source": [
    "# to inspect the retrieved documents \n",
    "print(\"\\n--- Retrieved Documents ---\")\n",
    "for doc in response['context']:\n",
    "    print(doc.page_content[:200], \"...\") # Print first 200 chars\n",
    "    print(\"---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3263deca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ollama LLM Tinyllama initialized\n"
     ]
    }
   ],
   "source": [
    "from langchain_ollama import OllamaLLM\n",
    "from langchain_core.prompts import ChatPromptTemplate #chat based prompts\n",
    "from langchain_core.output_parsers import StrOutputParser #raw to usable format out\n",
    "\n",
    "from langchain.chains.combine_documents import create_stuff_documents_chain \n",
    "#multiple chunks into single usable input \n",
    "from langchain.chains import create_retrieval_chain #to orchestrate entire rag pipeline\n",
    "\n",
    "llm=OllamaLLM(model=\"tinyllama\")\n",
    "print(\"Ollama LLM Tinyllama initialized\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a92c8496",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RAG PROMPT TEMPLATE DEFINED\n"
     ]
    }
   ],
   "source": [
    "rag_prompt=ChatPromptTemplate.from_messages([\n",
    "    (\"system\",\"you are a helpful AI assistant. Use only the following provided  provided context to answer the question. If the answer is not in the context, clearly state 'I don't know' or 'Answer not in the provided document'.\\n\\nContext:\\n{context}\"),\n",
    "    (\"user\", \"{input}\")\n",
    "])\n",
    "print(\"RAG PROMPT TEMPLATE DEFINED\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "bf7f7087",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retriever created from Chroma DB.\n"
     ]
    }
   ],
   "source": [
    "# retriever from vector store and return top k matching results\n",
    "# This object is what the retrieval chain will use to perform similarity searches\n",
    "retriever = db.as_retriever() \n",
    "#method to transform vector store to retreiver\n",
    "print(\"Retriever created from Chroma DB.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "897f39c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document combination chain created.\n",
      "Full RAG retrieval chain created.\n"
     ]
    }
   ],
   "source": [
    "# Create the Document Combination Chain\n",
    "# This chain takes the retrieved documents and stuffs them into the LLM's context\n",
    "document_chain = create_stuff_documents_chain(llm, rag_prompt)\n",
    "print(\"Document combination chain created.\")\n",
    "\n",
    "# Creating the full RAG Retrieval Chain\n",
    "# chain to orchestrates the retrieval and generation steps\n",
    "retrieval_chain = create_retrieval_chain(retriever, document_chain)\n",
    "print(\"Full RAG retrieval chain created.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "85c11d72",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Invoking the RAG Chain ---\n"
     ]
    }
   ],
   "source": [
    "# Invoking the RAG Chain with the Query \n",
    "print(\"\\n--- Invoking the RAG Chain ---\")\n",
    "query_text = \"tell us about the LQ RAG?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0ddf5e42",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Your Question:\n",
      "tell us about the LQ RAG?\n",
      "\n",
      "AI's Answer:\n",
      "Question: What are the different components of the proposed legal quotation generation system, including the fine-tuning layers and evaluation agents in the RAG layer? Answer:\n",
      "\n",
      "Section III of the paper provides a detailed explanation for the various components of the proposed legal quotation generation system, including the fine-tuining layers and evaluation agents. The section introduces the Fine-Tuining (FT) layer, which involves fine-tuining both the embedding LLM and the generative LLM in a systematic way to optimize their performance. This is followed by the RAG layer, which includes advanced RAG modules, an evaluation agent, a prompt engineering agent, and a feedback mechanism to ensure the quality and accuracy of generated responses. The bottom left quadrant of the FT Layer illustrates human inputs, which are fed into the system to evaluate its performance and improve its capabilities.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#.invoke() method will run the entire RAG pipeline\n",
    "response = retrieval_chain.invoke({\"input\": query_text})\n",
    "\n",
    "print(f\"\\nYour Question:\\n{query_text}\")\n",
    "print(f\"\\nAI's Answer:\\n{response['answer']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "10ba4149",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Retrieved Documents ---\n",
      "adaptation, and improved adherence to instructions.\n",
      "4) Extensive evaluations were performed to assess the\n",
      "performance of the proposed RAG system. The results\n",
      "demonstrate that LQ-RAG consistently outpe ...\n",
      "---\n",
      "LQ-RAG is depicted in Algorithm 3.\n",
      "Algorithm 1Embedding LLM Fine-Tuning Process\n",
      "Constants: Loss function MNRL, Evaluator Eval,\n",
      "Learning Rate η\n",
      "Input: Csub-legal\n",
      "Output: Trained LLM network parameters  ...\n",
      "---\n",
      "of 80%, showing a 23% improvement over Naive RAG and\n",
      "a 14% improvement over RAG with FTM. This substantial\n",
      "improvement is attributed to the advanced integration and\n",
      "fine-tuning techniques in LQ-RAG, w ...\n",
      "---\n",
      "proposed LQ-RAG system. The proposed system is organized\n",
      "into two primary parts: Fine-Tuning (FT) Layer and RAG\n",
      "Layer. The FT Layer involves fine-tuning both the embedding\n",
      "LLM and the generative LLM.  ...\n",
      "---\n"
     ]
    }
   ],
   "source": [
    "# to inspect the retrieved documents \n",
    "print(\"\\n--- Retrieved Documents ---\")\n",
    "for doc in response['context']:\n",
    "    print(doc.page_content[:200], \"...\") # Print first 200 chars\n",
    "    print(\"---\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "langchain (3.10.0)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
